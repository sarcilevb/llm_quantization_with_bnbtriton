{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qqq -U transformers datasets accelerate bitsandbytes --progress-bar off\n",
    "!pip install -qqq -U triton --upgrade --progress-bar off"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашняя работа"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Сжатие и ускорению работы LLM с помощью квантизации W8A8-INT**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В данной домашней работе мы выполним квантизацию LLM путем замены линейных слоев на свой линейный слой, в котором веса оригинальной модели квантизованны в `int8`, а во время расчета  `forward` котором выполняется квантизация активаций в `int8`. Для создания своего линейного слоя будем использовать triton kernels от `bitsandbytes`. </br>\n",
    "\n",
    "План работы:\n",
    "1. Напишем функцию для квантизации матриц на `pytorch` и сравним ее эффективность с функцией, использующую triton kernels.\n",
    "2. Напишем функцию на `pytorch`, в котором выполняется перемножение целочисленных матриц, а результат деквантизуется в float16. \n",
    "3. На основе анализа скорости напишем свой квантизованный слой с использованием тех функций, которые работают быстрее. \n",
    "4. Квантизуем LLM путем замены слоев на квантизованные. Выполним анализ скорости вычислений и генерирующей способности модели после квантизации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from typing import Optional, Union, Tuple\n",
    "\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import transformers\n",
    "\n",
    "from bnbtriton.quantize_rowwise import quantize_rowwise\n",
    "from bnbtriton.int8_matmul_rowwise_dequantize import int8_matmul_rowwise_dequantize\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    LlamaTokenizer,\n",
    "    LlamaTokenizerFast\n",
    ")\n",
    "# from huggingface_hub import snapshot_download\n",
    "\n",
    "# cache директория для хранения файлов, загруженных с hf\n",
    "# os.environ[\"HF_HOME\"] = \"/content/hf_cache\"\n",
    "# os.environ[\"TRANSFORMERS_CACHE\"]= \"/content/hf_cache\"\n",
    "\n",
    "def print_memory():\n",
    "    # Функция измерения затраченной GPU памяти\n",
    "    device='cuda'\n",
    "    mem_allocated = torch.cuda.memory_allocated(device=device) / 1024**3\n",
    "    mem_reserved = torch.cuda.memory_allocated(device=device) / 1024**3\n",
    "    print(f\"allocated: {mem_allocated:,.2f} gb\")\n",
    "    print(f\" reserved: {mem_reserved:,.2f} gb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_pytorch_function(func, input):\n",
    "    # Функция для имерения скорости расчета `func` для входа `input`\n",
    "\n",
    "    # CUDA IS ASYNC so can't use python time module\n",
    "    start = torch.cuda.Event(enable_timing=True)\n",
    "    end = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "    # Warmup\n",
    "    for _ in range(5):\n",
    "        func(*input)\n",
    "\n",
    "    start.record()\n",
    "    func(*input)\n",
    "    end.record()\n",
    "    torch.cuda.synchronize()\n",
    "    \n",
    "    return start.elapsed_time(end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bitsandbytes Triton kernels "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используя методы pytorch, реализуем симметричную квантизацию c построчным параметром масштабирования матрицы `W` в int8, результат сравним с результатом применения функции `quantize_rowwise`.\n",
    "\n",
    "Параметр масштабирования вычисляется для каждой строки матрицы `W` по формуле: <br>\n",
    "$\\alpha = \\max(\\vert x \\vert)$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = torch.randn((11008, 4096)).to(dtype=torch.float16, device=torch.device('cuda:0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_quantize_rowwise(W: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    quantfactor = 127.0\n",
    "    # Здесь ваш код\n",
    "    return W_int8, W_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_int8_torch, W_scale_torch = torch_quantize_rowwise(W)\n",
    "W_int8_bnb, W_scale_bnb = quantize_rowwise(W)\n",
    "\n",
    "assert torch.allclose(W_int8_torch, W_int8_bnb, atol=1.0), 'Quantized matrices do not match'\n",
    "assert torch.allclose(W_scale_torch, W_scale_bnb), 'Scales do not match'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подсказка: \n",
    "\n",
    "\n",
    "если результаты не совпадают попробуйте перевести значение матрицы и параметров масштабирования в fp32 перед делением."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Измерить скорость выполнения `torch_quantize_rowwise` и `quantize_rowwise`. Какая функция работает быстрее?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0336639881134033"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_pytorch_function(#здесь ваш код)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5888000130653381"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_pytorch_function(#здесь ваш код)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С помощью целочисленного матричного умножения `torch._int_mm` реализовать функцию перемножения двух матриц в int8 с последующей деквантизацией результата умножения в fp16. \n",
    "\n",
    "Если необходимо, то формула для вычисления может быть найдена в работе [LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale](https://arxiv.org/pdf/2208.07339)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.randn((2048, 4096)).to(dtype=torch.float16, device=torch.device('cuda:0'))\n",
    "bias = torch.randn(11008).to(dtype=torch.float16, device=torch.device('cuda:0'))\n",
    "X_int8_torch, X_scale_torch = torch_quantize_rowwise(X)\n",
    "X_int8_bnb, X_scale_bnb = quantize_rowwise(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_int8_matmul_rowwise_dequantize(\n",
    "    X_int8_torch, \n",
    "    W_int8_torch_transpose, \n",
    "    X_scale_torch,\n",
    "    W_scale_torch,\n",
    "    bias = None\n",
    "):\n",
    "    divfactor = 1.0 / (127.0 * 127.0)\n",
    "    #здесь ваш код\n",
    "\n",
    "    if bias is not None:\n",
    "        #здесь ваш код\n",
    "\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для того чтобы сравнить производительность нашей функции с функцией, использующей triton kernels, нам необходимо добавить код для деквантизации в функцию `bnbtriton/src/bnbtriton/int8_matmul_rowwise_dequantize.py` </br>\n",
    "Если возникнут сложности, то код для добавления может быть найден в репозитории `https://github.com/bitsandbytes-foundation/bitsandbytes/tree/main/bitsandbytes`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_torch = torch_int8_matmul_rowwise_dequantize(\n",
    "    X_int8_torch, \n",
    "    W_int8_torch.t(), \n",
    "    X_scale_torch,\n",
    "    W_scale_torch,\n",
    "    bias \n",
    ")\n",
    "\n",
    "out_bnb = int8_matmul_rowwise_dequantize(\n",
    "    X_int8_torch, \n",
    "    W_int8_torch.t(), \n",
    "    X_scale_torch,\n",
    "    W_scale_torch,\n",
    "    bias\n",
    ")\n",
    "\n",
    "assert torch.allclose(out_torch, out_bnb), 'Matmul outputs do not match'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_torch = torch_int8_matmul_rowwise_dequantize(\n",
    "    X_int8_bnb, \n",
    "    W_int8_bnb.t(), \n",
    "    X_scale_torch,\n",
    "    W_scale_torch,\n",
    "    bias \n",
    ")\n",
    "\n",
    "out_bnb = int8_matmul_rowwise_dequantize(\n",
    "    X_int8_bnb, \n",
    "    W_int8_bnb.t(), \n",
    "    X_scale_torch,\n",
    "    W_scale_torch,\n",
    "    bias\n",
    ")\n",
    "\n",
    "assert torch.allclose(out_torch, out_bnb), 'Matmul outputs do not match'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Воспользуемся функцией `time_pytorch_function` для того, чтобы измерить скорость матричного произведения посредством `torch_int8_matmul_rowwise_dequantize`, `int8_matmul_rowwise_dequantize`.\n",
    "Выполним замеры с `bias` и без него.\n",
    "\n",
    "Какой метод работает быстрее?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.288640022277832"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_pytorch_function(#здесь ваш код)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.954367995262146"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_pytorch_function(#здесь ваш код)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В дополнение к этому сравним полученные результаты с измерениями для функции `torch.nn.functional.linear` для умножения матриц `X` и `W` в fp16.\n",
    "\n",
    "В каком формате (fp16 или int8 + деквантизация) быстрее выполняется произведение матриц?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5360000133514404"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_pytorch_function(#здесь ваш код)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Квантизация LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загружаем LLM. В качестве примера взят `Mistral-7B`. <br>\n",
    "Если памяти GPU недостаточно можно взять модель меньшего размера, например, `TinyLlama`. <br>\n",
    "\n",
    "Код проверен для llama подобных моделей. Поэтому если архитектура будет отличаться, то могут потребоваться небольшие корректировки кода, на этапе замены линейных слоев.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Имя модели на hf\n",
    "# model_name = \"mistralai/Mistral-7B-v0.3\"\n",
    "# model_name = \"NousResearch/Llama-2-7b-hf\"\n",
    "# model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "\n",
    "# mistral_models_path = Path('/content').joinpath('Mistral-7B-v0.3')\n",
    "model_path = Path('/home').joinpath(\"/home/LLaMA/huggingface/Mistral-7B-v0.3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка модели\n",
    "# Если для загрузки модели требуется токен hf_token, то предварительно записываем его \n",
    "# в файл hf_token.txt\n",
    "\n",
    "# with open(\"./hf_token.txt\", \"r\") as f:\n",
    "#     hf_token = f.read()\n",
    "# os.environ[\"HF_TOKEN\"] = hf_token\n",
    "\n",
    "# model_path.mkdir(parents=True, exist_ok=True)\n",
    "# snapshot_download(\n",
    "#     repo_id=\"mistralai/Mistral-7B-v0.3\",\n",
    "#     local_dir=model_path,\n",
    "#     allow_patterns=[\n",
    "#         \"params.json\",\n",
    "#         \"config.json\",\n",
    "#         \"model.safetensors.index.json\",\n",
    "#         \"model-00001-of-00003.safetensors\",\n",
    "#         \"model-00002-of-00003.safetensors\",\n",
    "#         \"model-00003-of-00003.safetensors\",\n",
    "#         \"tokenizer.json\",\n",
    "#         \"tokenizer_config.json\",\n",
    "#         \"special_tokens_map.json\",\n",
    "#         \"tokenizer.model\"]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:06<00:00,  2.16s/it]\n"
     ]
    }
   ],
   "source": [
    "# Загрузка предобученной модели\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=torch.float16,\n",
    "    trust_remote_code = True,\n",
    "    device_map = 'cuda:0'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "if not tokenizer.pad_token_id:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MistralForCausalLM(\n",
       "  (model): MistralModel(\n",
       "    (embed_tokens): Embedding(32768, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x MistralDecoderLayer(\n",
       "        (self_attn): MistralSdpaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): MistralRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): MistralMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): MistralRMSNorm()\n",
       "        (post_attention_layernorm): MistralRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): MistralRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32768, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "allocated: 14.00 gb\n",
      " reserved: 14.00 gb\n"
     ]
    }
   ],
   "source": [
    "print_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим генерирующие способности модели, путем генерации ответов на два вопроса."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "questions = [\n",
    "    \"Как дела?\",\n",
    "]\n",
    "\n",
    "answers = []\n",
    "\n",
    "for question in questions:\n",
    "    tokenized_input = tokenizer(\n",
    "        f\"QUESTION: {question}\\n ANSWER:\",\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            **tokenized_input,\n",
    "            max_length=50, num_beams=3, early_stopping=True,\n",
    "        )[0]\n",
    "    answer = tokenizer.decode(output, skip_special_tokens=True)\n",
    "    answers.append(answer[:answer.find(\".\")] + \".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['QUESTION: What is result of 2^5?\\n ANSWER: 32\\n\\nQUESTION: What is result of 2^6?\\n ANSWER: 64\\n\\nQUESTIO.',\n",
       " 'QUESTION: Как добраться до Сколтеха?\\n ANSWER: Сколтех находится в 10 минутах ходьбы от станции метро «Сокольник.']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Измерим уровень перплексии на датасете wikitext2-test. Также измерим время, затраченное на расчет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Load and process wikitext2 dataset\n",
    "def get_wikitext2(nsamples=128, seed=0, seqlen=2048, tokenizer=None):\n",
    "    # Load test datasets\n",
    "    testdata = load_dataset('wikitext', 'wikitext-2-raw-v1', split='test')\n",
    "    testenc = tokenizer(\"\\n\\n\".join(testdata['text']), return_tensors='pt')\n",
    "    trainloader = None\n",
    "    return trainloader, testenc\n",
    "\n",
    "\n",
    "# Function to evaluate perplexity (ppl) specifically on the wikitext dataset\n",
    "def eval_ppl_wikitext(model, testenc, bs=1, device=None):\n",
    "    # Get input IDs\n",
    "    testenc = testenc.input_ids\n",
    "\n",
    "    # Calculate number of samples\n",
    "    nsamples = testenc.numel() // model.seqlen\n",
    "\n",
    "    # List to store negative log likelihoods\n",
    "    nlls = []\n",
    "    print(f\"nsamples {nsamples}\")\n",
    "\n",
    "    # Loop through each batch\n",
    "    for i in range(0,nsamples,bs):\n",
    "        if i % 50 == 0:\n",
    "            print(f\"sample {i}\")\n",
    "\n",
    "        # Calculate end index\n",
    "        j = min(i+bs, nsamples)\n",
    "\n",
    "        # Prepare inputs and move to device\n",
    "        inputs = testenc[:,(i * model.seqlen):(j * model.seqlen)].to(device)\n",
    "        inputs = inputs.reshape(j-i, model.seqlen)\n",
    "\n",
    "        # Forward pass through the model\n",
    "        lm_logits = model(inputs).logits\n",
    "\n",
    "        # Shift logits and labels for next token prediction\n",
    "        shift_logits = lm_logits[:, :-1, :].contiguous()\n",
    "        shift_labels = inputs[:, 1:]\n",
    "\n",
    "        # Compute loss\n",
    "        loss_fct = torch.nn.CrossEntropyLoss()\n",
    "        loss = loss_fct(shift_logits.reshape(-1, shift_logits.size(-1)), shift_labels.reshape(-1))\n",
    "\n",
    "        # Calculate negative log likelihood\n",
    "        neg_log_likelihood = loss.float() * model.seqlen * (j-i)\n",
    "\n",
    "        # Append to list of negative log likelihoods\n",
    "        nlls.append(neg_log_likelihood)\n",
    "\n",
    "    # Compute perplexity\n",
    "    ppl = torch.exp(torch.stack(nlls).sum() / (nsamples * model.seqlen))\n",
    "\n",
    "    # Empty CUDA cache to save memory\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return ppl.item()\n",
    "\n",
    "# Function to evaluate perplexity (ppl) on a specified model and tokenizer\n",
    "def eval_ppl(model, tokenizer, device=torch.device(\"cuda:0\")):\n",
    "    # Set dataset\n",
    "    dataset = \"wikitext2\"\n",
    "    model.seqlen = 2048\n",
    "\n",
    "    # Print status\n",
    "    print(f\"evaluating on {dataset}\")\n",
    "\n",
    "    # Get the test loader\n",
    "    _, testloader = get_wikitext2(seqlen=model.seqlen, tokenizer=tokenizer)\n",
    "\n",
    "    # Evaluate ppl in no grad context to avoid updating the model\n",
    "    with torch.no_grad():\n",
    "        ppl_test = eval_ppl_wikitext(model, testloader, 1, device)\n",
    "    return ppl_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating on wikitext2\n",
      "nsamples 163\n",
      "sample 0\n",
      "sample 50\n",
      "sample 100\n",
      "sample 150\n",
      "5.317403316497803\n",
      "69887.875\n"
     ]
    }
   ],
   "source": [
    "start = torch.cuda.Event(enable_timing=True)\n",
    "end = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "start.record()\n",
    "ppl = eval_ppl(model, tokenizer)\n",
    "end.record()\n",
    "torch.cuda.synchronize()\n",
    "print(ppl)\n",
    "print(start.elapsed_time(end))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BnbLinearW8A8OF16(torch.nn.Module):\n",
    "    '''\n",
    "    Линейный слой с квантизованными в int8 весами.\n",
    "    При расчете forward pass активации квантизуются в int8\n",
    "    '''\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features: int,\n",
    "        out_features: int,\n",
    "        bias: bool = True,\n",
    "        scale: Union[torch.tensor, float] = 1.0,\n",
    "        params_dtype: Optional[torch.dtype] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Keep input parameters\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        \n",
    "        self.register_buffer(\n",
    "            \"weight\",\n",
    "            torch.empty(\n",
    "                self.out_features,\n",
    "                self.in_features,\n",
    "                dtype=torch.int8,\n",
    "                requires_grad=False,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        if bias:\n",
    "            self.register_buffer(\n",
    "                \"bias\",\n",
    "                torch.empty(\n",
    "                    self.out_features,\n",
    "                    dtype=torch.float16,\n",
    "                    requires_grad=False,\n",
    "                ),                \n",
    "            )\n",
    "        else:\n",
    "            self.register_parameter(\"bias\", None)\n",
    "\n",
    "        # Одномерный массив параметров масштабирования для каждой строки матрицы весов\n",
    "        self.register_buffer(\"weight_scale\", torch.ones(out_features))\n",
    "    \n",
    "    def forward(self, X_3D):\n",
    "        X = X_3D.view(-1, X_3D.size(-1))\n",
    "\n",
    "        # Квантизовать входные активации X, используя функцию `quantize_rowwise`\n",
    "        # здесь ваш код\n",
    "\n",
    "        # Вычислить произведение весов на активации с \n",
    "        # использованием `int8_matmul_rowwise_dequantize`\n",
    "        res = #здесь ваш код.view(*X_3D.size()[:-1], -1)\n",
    "        \n",
    "        return res\n",
    "\n",
    "    @classmethod\n",
    "    def from_linear(\n",
    "        cls,\n",
    "        linear: torch.nn.Linear\n",
    "    ):\n",
    "        q_linear = cls(\n",
    "            linear.in_features,\n",
    "            linear.out_features,\n",
    "            linear.bias is not None,\n",
    "        )\n",
    "\n",
    "        if linear.bias is not None:\n",
    "            q_linear.bias = linear.bias.clone().half()\n",
    "\n",
    "        linear_weight = linear.weight.data.clone()\n",
    "        # Квантизовать веса linear_weight в int8\n",
    "        #здесь ваш код\n",
    "\n",
    "        assert (\n",
    "            linear_weight.min() >= -128 and \n",
    "            linear_weight.max() <= 127\n",
    "        ), \"Quantized weight out of range\"\n",
    "\n",
    "        q_linear.weight_scale = weight_scale.contiguous()\n",
    "        q_linear.weight.data = linear_weight.contiguous()\n",
    "\n",
    "        return q_linear\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'W8A8Linear({self.in_features}, {self.out_features}, bias={self.bias is not None})'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Допишем процедуру `replace_with_qlinear` для замены линейных слоев в нашей LLM\n",
    "на квантизованные слои. Блоки `embed_tokens` и `lm_head` оставляем без изменений."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_with_qlinear(root_module):\n",
    "    '''\n",
    "    Процедура для замены линейных слоев в блоках трансформеров модели \n",
    "    на квантизованные линейные слои BnbLinearW8A8OF16\n",
    "    '''\n",
    "\n",
    "    module_name_dict = {name: module for name, module in root_module.named_modules()}\n",
    "    for name, module in module_name_dict.items():\n",
    "        if isinstance(module, torch.nn.Linear):\n",
    "            ind = name.rfind(\".\")\n",
    "            if ind == -1:\n",
    "                father = module_name_dict[\"\"]\n",
    "            else:\n",
    "                father = module_name_dict[name[:ind]]\n",
    "\n",
    "            #здесь ваш код\n",
    "\n",
    "            setattr(father, name[ind + 1 :], q_linear)\n",
    "            print(f\"replace layer {name} with {q_linear}\")\n",
    "            del module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "replace layer layers.0.self_attn.q_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.0.self_attn.k_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.0.self_attn.v_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.0.self_attn.o_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.0.mlp.gate_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.0.mlp.up_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.0.mlp.down_proj with W8A8Linear(14336, 4096, bias=False)\n",
      "replace layer layers.1.self_attn.q_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.1.self_attn.k_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.1.self_attn.v_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.1.self_attn.o_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.1.mlp.gate_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.1.mlp.up_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.1.mlp.down_proj with W8A8Linear(14336, 4096, bias=False)\n",
      "replace layer layers.2.self_attn.q_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.2.self_attn.k_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.2.self_attn.v_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.2.self_attn.o_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.2.mlp.gate_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.2.mlp.up_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.2.mlp.down_proj with W8A8Linear(14336, 4096, bias=False)\n",
      "replace layer layers.3.self_attn.q_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.3.self_attn.k_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.3.self_attn.v_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.3.self_attn.o_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.3.mlp.gate_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.3.mlp.up_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.3.mlp.down_proj with W8A8Linear(14336, 4096, bias=False)\n",
      "replace layer layers.4.self_attn.q_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.4.self_attn.k_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.4.self_attn.v_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.4.self_attn.o_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.4.mlp.gate_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.4.mlp.up_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.4.mlp.down_proj with W8A8Linear(14336, 4096, bias=False)\n",
      "replace layer layers.5.self_attn.q_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.5.self_attn.k_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.5.self_attn.v_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.5.self_attn.o_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.5.mlp.gate_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.5.mlp.up_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.5.mlp.down_proj with W8A8Linear(14336, 4096, bias=False)\n",
      "replace layer layers.6.self_attn.q_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.6.self_attn.k_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.6.self_attn.v_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.6.self_attn.o_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.6.mlp.gate_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.6.mlp.up_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.6.mlp.down_proj with W8A8Linear(14336, 4096, bias=False)\n",
      "replace layer layers.7.self_attn.q_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.7.self_attn.k_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.7.self_attn.v_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.7.self_attn.o_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.7.mlp.gate_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.7.mlp.up_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.7.mlp.down_proj with W8A8Linear(14336, 4096, bias=False)\n",
      "replace layer layers.8.self_attn.q_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.8.self_attn.k_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.8.self_attn.v_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.8.self_attn.o_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.8.mlp.gate_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.8.mlp.up_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.8.mlp.down_proj with W8A8Linear(14336, 4096, bias=False)\n",
      "replace layer layers.9.self_attn.q_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.9.self_attn.k_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.9.self_attn.v_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.9.self_attn.o_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.9.mlp.gate_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.9.mlp.up_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.9.mlp.down_proj with W8A8Linear(14336, 4096, bias=False)\n",
      "replace layer layers.10.self_attn.q_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.10.self_attn.k_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.10.self_attn.v_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.10.self_attn.o_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.10.mlp.gate_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.10.mlp.up_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.10.mlp.down_proj with W8A8Linear(14336, 4096, bias=False)\n",
      "replace layer layers.11.self_attn.q_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.11.self_attn.k_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.11.self_attn.v_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.11.self_attn.o_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.11.mlp.gate_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.11.mlp.up_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.11.mlp.down_proj with W8A8Linear(14336, 4096, bias=False)\n",
      "replace layer layers.12.self_attn.q_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.12.self_attn.k_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.12.self_attn.v_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.12.self_attn.o_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.12.mlp.gate_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.12.mlp.up_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.12.mlp.down_proj with W8A8Linear(14336, 4096, bias=False)\n",
      "replace layer layers.13.self_attn.q_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.13.self_attn.k_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.13.self_attn.v_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.13.self_attn.o_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.13.mlp.gate_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.13.mlp.up_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.13.mlp.down_proj with W8A8Linear(14336, 4096, bias=False)\n",
      "replace layer layers.14.self_attn.q_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.14.self_attn.k_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.14.self_attn.v_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.14.self_attn.o_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.14.mlp.gate_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.14.mlp.up_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.14.mlp.down_proj with W8A8Linear(14336, 4096, bias=False)\n",
      "replace layer layers.15.self_attn.q_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.15.self_attn.k_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.15.self_attn.v_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.15.self_attn.o_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.15.mlp.gate_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.15.mlp.up_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.15.mlp.down_proj with W8A8Linear(14336, 4096, bias=False)\n",
      "replace layer layers.16.self_attn.q_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.16.self_attn.k_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.16.self_attn.v_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.16.self_attn.o_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.16.mlp.gate_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.16.mlp.up_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.16.mlp.down_proj with W8A8Linear(14336, 4096, bias=False)\n",
      "replace layer layers.17.self_attn.q_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.17.self_attn.k_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.17.self_attn.v_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.17.self_attn.o_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.17.mlp.gate_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.17.mlp.up_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.17.mlp.down_proj with W8A8Linear(14336, 4096, bias=False)\n",
      "replace layer layers.18.self_attn.q_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.18.self_attn.k_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.18.self_attn.v_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.18.self_attn.o_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.18.mlp.gate_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.18.mlp.up_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.18.mlp.down_proj with W8A8Linear(14336, 4096, bias=False)\n",
      "replace layer layers.19.self_attn.q_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.19.self_attn.k_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.19.self_attn.v_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.19.self_attn.o_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.19.mlp.gate_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.19.mlp.up_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.19.mlp.down_proj with W8A8Linear(14336, 4096, bias=False)\n",
      "replace layer layers.20.self_attn.q_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.20.self_attn.k_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.20.self_attn.v_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.20.self_attn.o_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.20.mlp.gate_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.20.mlp.up_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.20.mlp.down_proj with W8A8Linear(14336, 4096, bias=False)\n",
      "replace layer layers.21.self_attn.q_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.21.self_attn.k_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.21.self_attn.v_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.21.self_attn.o_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.21.mlp.gate_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.21.mlp.up_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.21.mlp.down_proj with W8A8Linear(14336, 4096, bias=False)\n",
      "replace layer layers.22.self_attn.q_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.22.self_attn.k_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.22.self_attn.v_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.22.self_attn.o_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.22.mlp.gate_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.22.mlp.up_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.22.mlp.down_proj with W8A8Linear(14336, 4096, bias=False)\n",
      "replace layer layers.23.self_attn.q_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.23.self_attn.k_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.23.self_attn.v_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.23.self_attn.o_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.23.mlp.gate_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.23.mlp.up_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.23.mlp.down_proj with W8A8Linear(14336, 4096, bias=False)\n",
      "replace layer layers.24.self_attn.q_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.24.self_attn.k_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.24.self_attn.v_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.24.self_attn.o_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.24.mlp.gate_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.24.mlp.up_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.24.mlp.down_proj with W8A8Linear(14336, 4096, bias=False)\n",
      "replace layer layers.25.self_attn.q_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.25.self_attn.k_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.25.self_attn.v_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.25.self_attn.o_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.25.mlp.gate_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.25.mlp.up_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.25.mlp.down_proj with W8A8Linear(14336, 4096, bias=False)\n",
      "replace layer layers.26.self_attn.q_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.26.self_attn.k_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.26.self_attn.v_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.26.self_attn.o_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.26.mlp.gate_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.26.mlp.up_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.26.mlp.down_proj with W8A8Linear(14336, 4096, bias=False)\n",
      "replace layer layers.27.self_attn.q_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.27.self_attn.k_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.27.self_attn.v_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.27.self_attn.o_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.27.mlp.gate_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.27.mlp.up_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.27.mlp.down_proj with W8A8Linear(14336, 4096, bias=False)\n",
      "replace layer layers.28.self_attn.q_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.28.self_attn.k_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.28.self_attn.v_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.28.self_attn.o_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.28.mlp.gate_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.28.mlp.up_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.28.mlp.down_proj with W8A8Linear(14336, 4096, bias=False)\n",
      "replace layer layers.29.self_attn.q_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.29.self_attn.k_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.29.self_attn.v_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.29.self_attn.o_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.29.mlp.gate_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.29.mlp.up_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.29.mlp.down_proj with W8A8Linear(14336, 4096, bias=False)\n",
      "replace layer layers.30.self_attn.q_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.30.self_attn.k_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.30.self_attn.v_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.30.self_attn.o_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.30.mlp.gate_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.30.mlp.up_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.30.mlp.down_proj with W8A8Linear(14336, 4096, bias=False)\n",
      "replace layer layers.31.self_attn.q_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.31.self_attn.k_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.31.self_attn.v_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.31.self_attn.o_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.31.mlp.gate_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.31.mlp.up_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.31.mlp.down_proj with W8A8Linear(14336, 4096, bias=False)\n"
     ]
    }
   ],
   "source": [
    "replace_with_qlinear(model.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MistralForCausalLM(\n",
       "  (model): MistralModel(\n",
       "    (embed_tokens): Embedding(32768, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x MistralDecoderLayer(\n",
       "        (self_attn): MistralSdpaAttention(\n",
       "          (q_proj): W8A8Linear(4096, 4096, bias=False)\n",
       "          (k_proj): W8A8Linear(4096, 1024, bias=False)\n",
       "          (v_proj): W8A8Linear(4096, 1024, bias=False)\n",
       "          (o_proj): W8A8Linear(4096, 4096, bias=False)\n",
       "          (rotary_emb): MistralRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): MistralMLP(\n",
       "          (gate_proj): W8A8Linear(4096, 14336, bias=False)\n",
       "          (up_proj): W8A8Linear(4096, 14336, bias=False)\n",
       "          (down_proj): W8A8Linear(14336, 4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): MistralRMSNorm()\n",
       "        (post_attention_layernorm): MistralRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): MistralRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32768, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С помощью функции `print_memory` измерим изменение потребления памяти GPU после квантизации модели.\n",
    "\n",
    "Как изменилось потребление памяти?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "allocated: 7.79 gb\n",
      " reserved: 7.79 gb\n"
     ]
    }
   ],
   "source": [
    "print_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим ответы модели на те же самые вопросы `questions` после квантизации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "answers_quant = []\n",
    "\n",
    "#здесь ваш код"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['QUESTION: What is result of 2^5?\\n ANSWER: 32\\n\\nQUESTION: What is result of 2^6?\\n ANSWER: 64\\n\\nQUESTIO.',\n",
       " 'QUESTION: Как добраться до Сколтеха?\\n ANSWER: Сколтех находится в 10 минутах ходьбы от станции метро «Славянски.']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers_quant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Измерим уровень перплексии и скорость ее расчета для квантизованной модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating on wikitext2\n",
      "nsamples 163\n",
      "sample 0\n",
      "sample 50\n",
      "sample 100\n",
      "sample 150\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "59815.22265625"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#здесь ваш код"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как изменилась перплексия и скорость расчета бенчмарка после квантизации?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверьте изменится ли скорость расчета бенчмарка если квантизовать только линейные слои в `mlp` блоках трансформера или в `self_attn` блоках трансформера."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
