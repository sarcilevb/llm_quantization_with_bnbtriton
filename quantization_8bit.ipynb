{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашняя работа"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Сжатие и ускорению работы LLM с помощью квантизации W8A8-INT**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В данной домашней работе мы выполним квантизацию LLM путем замены линейных слоев на свой линейный слой, в котором веса оригинальной модели квантизованны в `int8`, а во время расчета  `forward` котором выполняется квантизация активаций в `int8`. Для создания своего линейного слоя будем использовать triton kernels от `bitsandbytes`. </br>\n",
    "\n",
    "План работы:\n",
    "1. Напишем функцию для квантизации матриц на `pytorch` и сравним ее эффективность с функцией, использующую triton kernels.\n",
    "2. Напишем функцию на `pytorch`, в котором выполняется перемножение целочисленных матриц, а результат деквантизуется в float16. \n",
    "3. На основе анализа скорости напишем свой квантизованный слой с использованием тех функций, которые работают быстрее. \n",
    "4. Квантизуем LLM путем замены слоев на квантизованные. Выполним анализ скорости вычислений и генерирующей способности модели после квантизации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from typing import Optional, Union, Tuple\n",
    "\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import transformers\n",
    "\n",
    "from bnbtriton.quantize_rowwise import quantize_rowwise\n",
    "from bnbtriton.int8_matmul_rowwise_dequantize import int8_matmul_rowwise_dequantize\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    LlamaTokenizer,\n",
    "    LlamaTokenizerFast\n",
    ")\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "# cache директория для хранения файлов, загруженных с hf\n",
    "# os.environ[\"HF_HOME\"] = \"/content/hf_cache\"\n",
    "# os.environ[\"TRANSFORMERS_CACHE\"]= \"/content/hf_cache\"\n",
    "\n",
    "def print_memory():\n",
    "    # Функция измерения затраченной GPU памяти\n",
    "    device='cuda'\n",
    "    mem_allocated = torch.cuda.memory_allocated(device=device) / 1024**3\n",
    "    mem_reserved = torch.cuda.memory_allocated(device=device) / 1024**3\n",
    "    print(f\"allocated: {mem_allocated:,.2f} gb\")\n",
    "    print(f\" reserved: {mem_reserved:,.2f} gb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_pytorch_function(func, input):\n",
    "    # Функция для имерения скорости расчета `func` для входа `input`\n",
    "\n",
    "    # CUDA IS ASYNC so can't use python time module\n",
    "    start = torch.cuda.Event(enable_timing=True)\n",
    "    end = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "    # Warmup\n",
    "    for _ in range(5):\n",
    "        func(*input)\n",
    "\n",
    "    start.record()\n",
    "    func(*input)\n",
    "    end.record()\n",
    "    torch.cuda.synchronize()\n",
    "    \n",
    "    return start.elapsed_time(end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bitsandbytes Triton kernels "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используя методы pytorch, реализуем симметричную квантизацию c построчным параметром масштабирования матрицы `W` в int8, результат сравним с результатом применения функции `quantize_rowwise`.\n",
    "\n",
    "Параметр масштабирования вычисляется для каждой строки матрицы `W` по формуле: <br>\n",
    "$\\alpha = \\max(\\vert x \\vert)$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = torch.randn((11008, 4096)).to(dtype=torch.float16, device=torch.device('cuda:0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_quantize_rowwise(W: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    quantfactor = 127.0\n",
    "    \n",
    "    W_scale = W.abs().amax(axis=1)\n",
    "    W_int8 = (W.to(torch.float32) / W_scale.to(torch.float32)[:, None] * quantfactor).to(torch.int8)\n",
    "    \n",
    "    return W_int8, W_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_int8_torch, W_scale_torch = torch_quantize_rowwise(W)\n",
    "W_int8_bnb, W_scale_bnb = quantize_rowwise(W)\n",
    "\n",
    "assert torch.allclose(W_int8_torch, W_int8_bnb, atol=1.0), 'Quantized matrices do not match'\n",
    "assert torch.allclose(W_scale_torch, W_scale_bnb), 'Scales do not match'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подсказка: \n",
    "\n",
    "\n",
    "если результаты не совпадают попробуйте перевести значение матрицы и параметров масштабирования в fp32 перед делением."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Измерить скорость выполнения `torch_quantize_rowwise` и `quantize_rowwise`. Какая функция работает быстрее?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.314879894256592"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_pytorch_function(torch_quantize_rowwise, (W,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5693439841270447"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_pytorch_function(quantize_rowwise, (W,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С помощью целочисленного матричного умножения `torch._int_mm` реализовать функцию перемножения двух матриц в int8 с последующей деквантизацией результата умножения в fp16. \n",
    "\n",
    "Если необходимо, то формула для вычисления может быть найдена в работе [LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale](https://arxiv.org/pdf/2208.07339)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.randn((2048, 4096)).to(dtype=torch.float16, device=torch.device('cuda:0'))\n",
    "bias = torch.randn(11008).to(dtype=torch.float16, device=torch.device('cuda:0'))\n",
    "X_int8_torch, X_scale_torch = torch_quantize_rowwise(X)\n",
    "X_int8_bnb, X_scale_bnb = quantize_rowwise(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_int8_matmul_rowwise_dequantize(\n",
    "    X_int8_torch, \n",
    "    W_int8_torch_transpose, \n",
    "    X_scale_torch,\n",
    "    W_scale_torch,\n",
    "    bias = None\n",
    "):\n",
    "    divfactor = 1.0 / (127.0 * 127.0)\n",
    "    result_int32 = torch._int_mm(X_int8_torch, W_int8_torch_transpose)\n",
    "    scales = X_scale_torch[:, None].to(torch.float32)  * W_scale_torch[None, :].to(torch.float32) \n",
    "    acc = (divfactor * scales * result_int32.to(torch.float32)).to(torch.float16)\n",
    "    if bias is not None:\n",
    "        acc += bias\n",
    "\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для того чтобы сравнить производительность нашей функции с функцией, использующей triton kernels, нам необходимо добавить код для деквантизации в функцию `bnbtriton/src/bnbtriton/int8_matmul_rowwise_dequantize.py` </br>\n",
    "Если возникнут сложности, то код для добавления может быть найден в репозитории `https://github.com/bitsandbytes-foundation/bitsandbytes/tree/main/bitsandbytes`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_torch = torch_int8_matmul_rowwise_dequantize(\n",
    "    X_int8_torch, \n",
    "    W_int8_torch.t(), \n",
    "    X_scale_torch,\n",
    "    W_scale_torch,\n",
    "    bias \n",
    ")\n",
    "\n",
    "out_bnb = int8_matmul_rowwise_dequantize(\n",
    "    X_int8_torch, \n",
    "    W_int8_torch.t(), \n",
    "    X_scale_torch,\n",
    "    W_scale_torch,\n",
    "    bias\n",
    ")\n",
    "\n",
    "assert torch.allclose(out_torch, out_bnb), 'Matmul outputs do not match'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_torch = torch_int8_matmul_rowwise_dequantize(\n",
    "    X_int8_bnb, \n",
    "    W_int8_bnb.t(), \n",
    "    X_scale_torch,\n",
    "    W_scale_torch,\n",
    "    bias \n",
    ")\n",
    "\n",
    "out_bnb = int8_matmul_rowwise_dequantize(\n",
    "    X_int8_bnb, \n",
    "    W_int8_bnb.t(), \n",
    "    X_scale_torch,\n",
    "    W_scale_torch,\n",
    "    bias\n",
    ")\n",
    "\n",
    "assert torch.allclose(out_torch, out_bnb), 'Matmul outputs do not match'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Воспользуемся функцией `time_pytorch_function` для того, чтобы измерить скорость матричного произведения посредством `torch_int8_matmul_rowwise_dequantize`, `int8_matmul_rowwise_dequantize`.\n",
    "Выполним замеры с `bias` и без него.\n",
    "\n",
    "Какой метод работает быстрее?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naive pytorch matmul times: w/bias 5.360640048980713, w/o/bias 5.001215934753418\n"
     ]
    }
   ],
   "source": [
    "time_with_bias = time_pytorch_function(torch_int8_matmul_rowwise_dequantize, (X_int8_bnb, W_int8_bnb.t(), X_scale_torch, W_scale_torch, bias,))\n",
    "time_wo_bias = time_pytorch_function(torch_int8_matmul_rowwise_dequantize, (X_int8_bnb, W_int8_bnb.t(), X_scale_torch, W_scale_torch, None,))\n",
    "print(f\"naive pytorch matmul times: w/bias {time_with_bias}, w/o/bias {time_wo_bias}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "triton matmul times: w/bias 1.45305597782135, w/o/bias 1.866752028465271\n"
     ]
    }
   ],
   "source": [
    "time_with_bias = time_pytorch_function(int8_matmul_rowwise_dequantize, (X_int8_bnb, W_int8_bnb.t(), X_scale_torch, W_scale_torch, bias,))\n",
    "time_wo_bias = time_pytorch_function(int8_matmul_rowwise_dequantize, (X_int8_bnb, W_int8_bnb.t(), X_scale_torch, W_scale_torch, None,))\n",
    "print(f\"triton matmul times: w/bias {time_with_bias}, w/o/bias {time_wo_bias}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В дополнение к этому сравним полученные результаты с измерениями для функции `torch.nn.functional.linear` для умножения матриц `X` и `W` в fp16.\n",
    "\n",
    "В каком формате (fp16 или int8 + деквантизация) быстрее выполняется произведение матриц?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "native pytorch fp16 times: w/bias 2.4391679763793945, w/o/bias 2.713599920272827\n"
     ]
    }
   ],
   "source": [
    "time_with_bias = time_pytorch_function(torch.nn.functional.linear, (X, W, bias,))\n",
    "time_wo_bias = time_pytorch_function(torch.nn.functional.linear, (X, W, None,))\n",
    "print(f\"native pytorch fp16 times: w/bias {time_with_bias}, w/o/bias {time_wo_bias}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Квантизация LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загружаем LLM. В качестве примера взят `Mistral-7B`. <br>\n",
    "Если памяти GPU недостаточно можно взять модель меньшего размера, например, `TinyLlama`. <br>\n",
    "\n",
    "Код проверен для llama подобных моделей. Поэтому если архитектура будет отличаться, то могут потребоваться небольшие корректировки кода, на этапе замены линейных слоев.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Имя модели на hf\n",
    "model_name = \"mistralai/Mistral-7B-v0.3\"\n",
    "# model_name = \"NousResearch/Llama-2-7b-hf\"\n",
    "# model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "\n",
    "# mistral_models_path = Path('/content').joinpath('Mistral-7B-v0.3')\n",
    "model_path = Path('/home').joinpath(\"/home/LLaMA/huggingface/Mistral-7B-v0.3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка модели\n",
    "# Если для загрузки модели требуется токен hf_token, то предварительно записываем его \n",
    "# в файл hf_token.txt\n",
    "\n",
    "# with open(\"./hf_token.txt\", \"r\") as f:\n",
    "#     hf_token = f.read()\n",
    "# os.environ[\"HF_TOKEN\"] = hf_token\n",
    "\n",
    "# model_path.mkdir(parents=True, exist_ok=True)\n",
    "# snapshot_download(\n",
    "#     repo_id=\"mistralai/Mistral-7B-v0.3\",\n",
    "#     local_dir=model_path,\n",
    "#     allow_patterns=[\n",
    "#         \"params.json\",\n",
    "#         \"config.json\",\n",
    "#         \"model.safetensors.index.json\",\n",
    "#         \"model-00001-of-00003.safetensors\",\n",
    "#         \"model-00002-of-00003.safetensors\",\n",
    "#         \"model-00003-of-00003.safetensors\",\n",
    "#         \"tokenizer.json\",\n",
    "#         \"tokenizer_config.json\",\n",
    "#         \"special_tokens_map.json\",\n",
    "#         \"tokenizer.model\"]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92397bee71074bd88a50193e6336992d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Загрузка предобученной модели\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=torch.float16,\n",
    "    trust_remote_code = True,\n",
    "    device_map = 'cuda:0'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "if not tokenizer.pad_token_id:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MistralForCausalLM(\n",
       "  (model): MistralModel(\n",
       "    (embed_tokens): Embedding(32768, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x MistralDecoderLayer(\n",
       "        (self_attn): MistralAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): MistralMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): MistralRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32768, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "allocated: 13.79 gb\n",
      " reserved: 13.79 gb\n"
     ]
    }
   ],
   "source": [
    "print_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим генерирующие способности модели, путем генерации ответов на два вопроса."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 15\u001b[0m\n\u001b[1;32m      9\u001b[0m tokenized_input \u001b[38;5;241m=\u001b[39m tokenizer(\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQUESTION: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquestion\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m ANSWER:\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     11\u001b[0m     return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     12\u001b[0m )\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 15\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[1;32m     16\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{k: v\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m tokenized_input\u001b[38;5;241m.\u001b[39mitems()},\n\u001b[1;32m     17\u001b[0m         max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, num_beams\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, early_stopping\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     18\u001b[0m     )[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     19\u001b[0m answer \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(output, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     20\u001b[0m answers\u001b[38;5;241m.\u001b[39mappend(answer[:answer\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "questions = [\n",
    "    \"Как пройти в библиотеку?\",\n",
    "    \"Кто виноват и что делать?\",\n",
    "]\n",
    "\n",
    "answers = []\n",
    "\n",
    "for question in questions:\n",
    "    tokenized_input = tokenizer(\n",
    "        f\"QUESTION: {question}\\n ANSWER:\",\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            **{k: v.to(\"cuda:0\") for k, v in tokenized_input.items()},\n",
    "            max_length=50, num_beams=3, early_stopping=True,\n",
    "        )[0]\n",
    "    answer = tokenizer.decode(output, skip_special_tokens=True)\n",
    "    answers.append(answer[:answer.find(\".\")] + \".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['QUESTION: Как пройти в библиотеку?\\n ANSWER: Вход в библиотеку осуществляется через главный вход, расположенный по адресу.',\n",
       " 'QUESTION: Кто виноват и что делать?\\n ANSWER: Нет виноватых и нечего делать.']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Измерим уровень перплексии на датасете wikitext2-test. Также измерим время, затраченное на расчет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Load and process wikitext2 dataset\n",
    "def get_wikitext2(nsamples=128, seed=0, seqlen=2048, tokenizer=None):\n",
    "    # Load test datasets\n",
    "    testdata = load_dataset('wikitext', 'wikitext-2-raw-v1', split='test')\n",
    "    testenc = tokenizer(\"\\n\\n\".join(testdata['text']), return_tensors='pt')\n",
    "    trainloader = None\n",
    "    return trainloader, testenc\n",
    "\n",
    "\n",
    "# Function to evaluate perplexity (ppl) specifically on the wikitext dataset\n",
    "def eval_ppl_wikitext(model, testenc, bs=1, device=None):\n",
    "    # Get input IDs\n",
    "    testenc = testenc.input_ids\n",
    "\n",
    "    # Calculate number of samples\n",
    "    nsamples = testenc.numel() // model.seqlen\n",
    "\n",
    "    # List to store negative log likelihoods\n",
    "    nlls = []\n",
    "    print(f\"nsamples {nsamples}\")\n",
    "\n",
    "    # Loop through each batch\n",
    "    for i in range(0,nsamples,bs):\n",
    "        if i % 50 == 0:\n",
    "            print(f\"sample {i}\")\n",
    "\n",
    "        # Calculate end index\n",
    "        j = min(i+bs, nsamples)\n",
    "\n",
    "        # Prepare inputs and move to device\n",
    "        inputs = testenc[:,(i * model.seqlen):(j * model.seqlen)].to(device)\n",
    "        inputs = inputs.reshape(j-i, model.seqlen)\n",
    "\n",
    "        # Forward pass through the model\n",
    "        lm_logits = model(inputs).logits\n",
    "\n",
    "        # Shift logits and labels for next token prediction\n",
    "        shift_logits = lm_logits[:, :-1, :].contiguous()\n",
    "        shift_labels = inputs[:, 1:]\n",
    "\n",
    "        # Compute loss\n",
    "        loss_fct = torch.nn.CrossEntropyLoss()\n",
    "        loss = loss_fct(shift_logits.reshape(-1, shift_logits.size(-1)), shift_labels.reshape(-1))\n",
    "\n",
    "        # Calculate negative log likelihood\n",
    "        neg_log_likelihood = loss.float() * model.seqlen * (j-i)\n",
    "\n",
    "        # Append to list of negative log likelihoods\n",
    "        nlls.append(neg_log_likelihood)\n",
    "\n",
    "    # Compute perplexity\n",
    "    ppl = torch.exp(torch.stack(nlls).sum() / (nsamples * model.seqlen))\n",
    "\n",
    "    # Empty CUDA cache to save memory\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return ppl.item()\n",
    "\n",
    "# Function to evaluate perplexity (ppl) on a specified model and tokenizer\n",
    "def eval_ppl(model, tokenizer, device=torch.device(\"cuda:0\")):\n",
    "    # Set dataset\n",
    "    dataset = \"wikitext2\"\n",
    "    model.seqlen = 2048\n",
    "\n",
    "    # Print status\n",
    "    print(f\"evaluating on {dataset}\")\n",
    "\n",
    "    # Get the test loader\n",
    "    _, testloader = get_wikitext2(seqlen=model.seqlen, tokenizer=tokenizer)\n",
    "\n",
    "    # Evaluate ppl in no grad context to avoid updating the model\n",
    "    with torch.no_grad():\n",
    "        ppl_test = eval_ppl_wikitext(model, testloader, 1, device)\n",
    "    return ppl_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating on wikitext2\n",
      "nsamples 163\n",
      "sample 0\n",
      "sample 50\n",
      "sample 100\n",
      "sample 150\n",
      "5.3174519538879395\n",
      "204066.328125\n"
     ]
    }
   ],
   "source": [
    "start = torch.cuda.Event(enable_timing=True)\n",
    "end = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "start.record()\n",
    "ppl = eval_ppl(model, tokenizer)\n",
    "end.record()\n",
    "torch.cuda.synchronize()\n",
    "print(ppl)\n",
    "print(start.elapsed_time(end))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BnbLinearW8A8OF16(torch.nn.Module):\n",
    "    '''\n",
    "    Линейный слой с квантизованными в int8 весами.\n",
    "    При расчете forward pass активации квантизуются в int8\n",
    "    '''\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features: int,\n",
    "        out_features: int,\n",
    "        bias: bool = True,\n",
    "        scale: Union[torch.tensor, float] = 1.0,\n",
    "        params_dtype: Optional[torch.dtype] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Keep input parameters\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        \n",
    "        self.register_buffer(\n",
    "            \"weight\",\n",
    "            torch.empty(\n",
    "                self.out_features,\n",
    "                self.in_features,\n",
    "                dtype=torch.int8,\n",
    "                requires_grad=False,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        if bias:\n",
    "            self.register_buffer(\n",
    "                \"bias\",\n",
    "                torch.empty(\n",
    "                    self.out_features,\n",
    "                    dtype=torch.float16,\n",
    "                    requires_grad=False,\n",
    "                ),                \n",
    "            )\n",
    "        else:\n",
    "            self.register_parameter(\"bias\", None)\n",
    "\n",
    "        # Одномерный массив параметров масштабирования для каждой строки матрицы весов\n",
    "        self.register_buffer(\"weight_scale\", torch.ones(out_features))\n",
    "    \n",
    "    def forward(self, X_3D):\n",
    "        X = X_3D.view(-1, X_3D.size(-1))\n",
    "\n",
    "        # Квантизовать входные активации X, используя функцию `quantize_rowwise`\n",
    "        X_int8, X_scale = quantize_rowwise(X)\n",
    "\n",
    "        # Вычислить произведение весов на активации с \n",
    "        # использованием `int8_matmul_rowwise_dequantize`\n",
    "        res = int8_matmul_rowwise_dequantize(\n",
    "            X_int8,\n",
    "            self.weight,\n",
    "            X_scale,\n",
    "            self.weight_scale,\n",
    "            self.bias,\n",
    "        )#здесь ваш код.view(*X_3D.size()[:-1], -1)\n",
    "        res = res.view(*X_3D.size()[:-1], -1)\n",
    "        \n",
    "        return res\n",
    "\n",
    "    @classmethod\n",
    "    def from_linear(\n",
    "        cls,\n",
    "        linear: torch.nn.Linear\n",
    "    ):\n",
    "        q_linear = cls(\n",
    "            linear.in_features,\n",
    "            linear.out_features,\n",
    "            linear.bias is not None,\n",
    "        )\n",
    "\n",
    "        if linear.bias is not None:\n",
    "            q_linear.bias = linear.bias.clone().half()\n",
    "\n",
    "        linear_weight = linear.weight.data.clone()\n",
    "        linear_weight, weight_scale = quantize_rowwise(linear_weight)\n",
    "        linear_weight = linear_weight.t()\n",
    "\n",
    "        assert (\n",
    "            linear_weight.min() >= -128 and \n",
    "            linear_weight.max() <= 127\n",
    "        ), \"Quantized weight out of range\"\n",
    "\n",
    "        q_linear.weight_scale = weight_scale.contiguous()\n",
    "        q_linear.weight.data = linear_weight.contiguous()\n",
    "\n",
    "        return q_linear\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'W8A8Linear({self.in_features}, {self.out_features}, bias={self.bias is not None})'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Допишем процедуру `replace_with_qlinear` для замены линейных слоев в нашей LLM\n",
    "на квантизованные слои. Блоки `embed_tokens` и `lm_head` оставляем без изменений."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_with_qlinear(root_module, quantize_mlp=True, quantize_selfattn=True, quantize_rest=True):\n",
    "    '''\n",
    "    Процедура для замены линейных слоев в блоках трансформеров модели \n",
    "    на квантизованные линейные слои BnbLinearW8A8OF16\n",
    "    '''\n",
    "\n",
    "    module_name_dict = {name: module for name, module in root_module.named_modules()}\n",
    "    for name, module in module_name_dict.items():\n",
    "        if isinstance(module, torch.nn.Linear) or isinstance(module, BnbLinearW8A8OF16):\n",
    "            ind = name.rfind(\".\")\n",
    "            if ind == -1:\n",
    "                father = module_name_dict[\"\"]\n",
    "            else:\n",
    "                father = module_name_dict[name[:ind]]\n",
    " \n",
    "            if \"mlp\" in name:\n",
    "                skip = not quantize_mlp\n",
    "            elif \"self_attn\" in name:\n",
    "                skip = not quantize_selfattn\n",
    "            else:\n",
    "                skip = not quantize_rest\n",
    "            if skip:\n",
    "                print(f\"skipping quantizing layer {name}\")\n",
    "                continue\n",
    "                    \n",
    "\n",
    "            #здесь ваш код\n",
    "            q_linear = BnbLinearW8A8OF16.from_linear(module)\n",
    "\n",
    "            setattr(father, name[ind + 1 :], q_linear)\n",
    "            print(f\"replace layer {name} with {q_linear}\")\n",
    "            del module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "replace layer layers.0.self_attn.q_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.0.self_attn.k_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.0.self_attn.v_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.0.self_attn.o_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.0.mlp.gate_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.0.mlp.up_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.0.mlp.down_proj with W8A8Linear(14336, 4096, bias=False)\n",
      "replace layer layers.1.self_attn.q_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.1.self_attn.k_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.1.self_attn.v_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.1.self_attn.o_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.1.mlp.gate_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.1.mlp.up_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.1.mlp.down_proj with W8A8Linear(14336, 4096, bias=False)\n",
      "replace layer layers.2.self_attn.q_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.2.self_attn.k_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.2.self_attn.v_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.2.self_attn.o_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.2.mlp.gate_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.2.mlp.up_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.2.mlp.down_proj with W8A8Linear(14336, 4096, bias=False)\n",
      "replace layer layers.3.self_attn.q_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.3.self_attn.k_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.3.self_attn.v_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.3.self_attn.o_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.3.mlp.gate_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.3.mlp.up_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.3.mlp.down_proj with W8A8Linear(14336, 4096, bias=False)\n",
      "replace layer layers.4.self_attn.q_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.4.self_attn.k_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.4.self_attn.v_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.4.self_attn.o_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.4.mlp.gate_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.4.mlp.up_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.4.mlp.down_proj with W8A8Linear(14336, 4096, bias=False)\n",
      "replace layer layers.5.self_attn.q_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.5.self_attn.k_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.5.self_attn.v_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.5.self_attn.o_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.5.mlp.gate_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.5.mlp.up_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.5.mlp.down_proj with W8A8Linear(14336, 4096, bias=False)\n",
      "replace layer layers.6.self_attn.q_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.6.self_attn.k_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.6.self_attn.v_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.6.self_attn.o_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.6.mlp.gate_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.6.mlp.up_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.6.mlp.down_proj with W8A8Linear(14336, 4096, bias=False)\n",
      "replace layer layers.7.self_attn.q_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.7.self_attn.k_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.7.self_attn.v_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.7.self_attn.o_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.7.mlp.gate_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.7.mlp.up_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.7.mlp.down_proj with W8A8Linear(14336, 4096, bias=False)\n",
      "replace layer layers.8.self_attn.q_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.8.self_attn.k_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.8.self_attn.v_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.8.self_attn.o_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.8.mlp.gate_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.8.mlp.up_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.8.mlp.down_proj with W8A8Linear(14336, 4096, bias=False)\n",
      "replace layer layers.9.self_attn.q_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.9.self_attn.k_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.9.self_attn.v_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.9.self_attn.o_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.9.mlp.gate_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.9.mlp.up_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.9.mlp.down_proj with W8A8Linear(14336, 4096, bias=False)\n",
      "replace layer layers.10.self_attn.q_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.10.self_attn.k_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.10.self_attn.v_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.10.self_attn.o_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.10.mlp.gate_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.10.mlp.up_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.10.mlp.down_proj with W8A8Linear(14336, 4096, bias=False)\n",
      "replace layer layers.11.self_attn.q_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.11.self_attn.k_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.11.self_attn.v_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.11.self_attn.o_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.11.mlp.gate_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.11.mlp.up_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.11.mlp.down_proj with W8A8Linear(14336, 4096, bias=False)\n",
      "replace layer layers.12.self_attn.q_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.12.self_attn.k_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.12.self_attn.v_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.12.self_attn.o_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.12.mlp.gate_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.12.mlp.up_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.12.mlp.down_proj with W8A8Linear(14336, 4096, bias=False)\n",
      "replace layer layers.13.self_attn.q_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.13.self_attn.k_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.13.self_attn.v_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.13.self_attn.o_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.13.mlp.gate_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.13.mlp.up_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.13.mlp.down_proj with W8A8Linear(14336, 4096, bias=False)\n",
      "replace layer layers.14.self_attn.q_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.14.self_attn.k_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.14.self_attn.v_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.14.self_attn.o_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.14.mlp.gate_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.14.mlp.up_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.14.mlp.down_proj with W8A8Linear(14336, 4096, bias=False)\n",
      "replace layer layers.15.self_attn.q_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.15.self_attn.k_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.15.self_attn.v_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.15.self_attn.o_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.15.mlp.gate_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.15.mlp.up_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.15.mlp.down_proj with W8A8Linear(14336, 4096, bias=False)\n",
      "replace layer layers.16.self_attn.q_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.16.self_attn.k_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.16.self_attn.v_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.16.self_attn.o_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.16.mlp.gate_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.16.mlp.up_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.16.mlp.down_proj with W8A8Linear(14336, 4096, bias=False)\n",
      "replace layer layers.17.self_attn.q_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.17.self_attn.k_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.17.self_attn.v_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.17.self_attn.o_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.17.mlp.gate_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.17.mlp.up_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.17.mlp.down_proj with W8A8Linear(14336, 4096, bias=False)\n",
      "replace layer layers.18.self_attn.q_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.18.self_attn.k_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.18.self_attn.v_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.18.self_attn.o_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.18.mlp.gate_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.18.mlp.up_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.18.mlp.down_proj with W8A8Linear(14336, 4096, bias=False)\n",
      "replace layer layers.19.self_attn.q_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.19.self_attn.k_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.19.self_attn.v_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.19.self_attn.o_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.19.mlp.gate_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.19.mlp.up_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.19.mlp.down_proj with W8A8Linear(14336, 4096, bias=False)\n",
      "replace layer layers.20.self_attn.q_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.20.self_attn.k_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.20.self_attn.v_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.20.self_attn.o_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.20.mlp.gate_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.20.mlp.up_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.20.mlp.down_proj with W8A8Linear(14336, 4096, bias=False)\n",
      "replace layer layers.21.self_attn.q_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.21.self_attn.k_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.21.self_attn.v_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.21.self_attn.o_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.21.mlp.gate_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.21.mlp.up_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.21.mlp.down_proj with W8A8Linear(14336, 4096, bias=False)\n",
      "replace layer layers.22.self_attn.q_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.22.self_attn.k_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.22.self_attn.v_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.22.self_attn.o_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.22.mlp.gate_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.22.mlp.up_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.22.mlp.down_proj with W8A8Linear(14336, 4096, bias=False)\n",
      "replace layer layers.23.self_attn.q_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.23.self_attn.k_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.23.self_attn.v_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.23.self_attn.o_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.23.mlp.gate_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.23.mlp.up_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.23.mlp.down_proj with W8A8Linear(14336, 4096, bias=False)\n",
      "replace layer layers.24.self_attn.q_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.24.self_attn.k_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.24.self_attn.v_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.24.self_attn.o_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.24.mlp.gate_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.24.mlp.up_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.24.mlp.down_proj with W8A8Linear(14336, 4096, bias=False)\n",
      "replace layer layers.25.self_attn.q_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.25.self_attn.k_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.25.self_attn.v_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.25.self_attn.o_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.25.mlp.gate_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.25.mlp.up_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.25.mlp.down_proj with W8A8Linear(14336, 4096, bias=False)\n",
      "replace layer layers.26.self_attn.q_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.26.self_attn.k_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.26.self_attn.v_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.26.self_attn.o_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.26.mlp.gate_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.26.mlp.up_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.26.mlp.down_proj with W8A8Linear(14336, 4096, bias=False)\n",
      "replace layer layers.27.self_attn.q_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.27.self_attn.k_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.27.self_attn.v_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.27.self_attn.o_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.27.mlp.gate_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.27.mlp.up_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.27.mlp.down_proj with W8A8Linear(14336, 4096, bias=False)\n",
      "replace layer layers.28.self_attn.q_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.28.self_attn.k_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.28.self_attn.v_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.28.self_attn.o_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.28.mlp.gate_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.28.mlp.up_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.28.mlp.down_proj with W8A8Linear(14336, 4096, bias=False)\n",
      "replace layer layers.29.self_attn.q_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.29.self_attn.k_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.29.self_attn.v_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.29.self_attn.o_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.29.mlp.gate_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.29.mlp.up_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.29.mlp.down_proj with W8A8Linear(14336, 4096, bias=False)\n",
      "replace layer layers.30.self_attn.q_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.30.self_attn.k_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.30.self_attn.v_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.30.self_attn.o_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.30.mlp.gate_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.30.mlp.up_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.30.mlp.down_proj with W8A8Linear(14336, 4096, bias=False)\n",
      "replace layer layers.31.self_attn.q_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.31.self_attn.k_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.31.self_attn.v_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.31.self_attn.o_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.31.mlp.gate_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.31.mlp.up_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.31.mlp.down_proj with W8A8Linear(14336, 4096, bias=False)\n"
     ]
    }
   ],
   "source": [
    "replace_with_qlinear(model.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MistralForCausalLM(\n",
       "  (model): MistralModel(\n",
       "    (embed_tokens): Embedding(32768, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x MistralDecoderLayer(\n",
       "        (self_attn): MistralAttention(\n",
       "          (q_proj): W8A8Linear(4096, 4096, bias=False)\n",
       "          (k_proj): W8A8Linear(4096, 1024, bias=False)\n",
       "          (v_proj): W8A8Linear(4096, 1024, bias=False)\n",
       "          (o_proj): W8A8Linear(4096, 4096, bias=False)\n",
       "        )\n",
       "        (mlp): MistralMLP(\n",
       "          (gate_proj): W8A8Linear(4096, 14336, bias=False)\n",
       "          (up_proj): W8A8Linear(4096, 14336, bias=False)\n",
       "          (down_proj): W8A8Linear(14336, 4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): MistralRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32768, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С помощью функции `print_memory` измерим изменение потребления памяти GPU после квантизации модели.\n",
    "\n",
    "Как изменилось потребление памяти?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "allocated: 0.00 gb\n",
      " reserved: 0.00 gb\n"
     ]
    }
   ],
   "source": [
    "print_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим ответы модели на те же самые вопросы `questions` после квантизации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "answers_quant = []\n",
    "\n",
    "for question in questions:\n",
    "    tokenized_input = tokenizer(\n",
    "        f\"QUESTION: {question}\\n ANSWER:\",\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            **{k: v.to(\"cuda:0\") for k, v in tokenized_input.items()},\n",
    "            max_length=50, num_beams=3, early_stopping=False,\n",
    "        )[0]\n",
    "    answer = tokenizer.decode(output, skip_special_tokens=True)\n",
    "    answers_quant.append(answer[:answer.find(\".\")] + \".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['QUESTION: Как пройти в библиотеку?\\n ANSWER: Вход в библиотеку осуществляется с 10:00 до 20:0.',\n",
       " 'QUESTION: Кто виноват и что делать?\\n ANSWER: Нет виноватых и нечего делать.']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers_quant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Измерим уровень перплексии и скорость ее расчета для квантизованной модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating on wikitext2\n",
      "nsamples 163\n",
      "sample 0\n",
      "sample 50\n",
      "sample 100\n",
      "sample 150\n",
      "5.3877129554748535\n",
      "151944.4375\n"
     ]
    }
   ],
   "source": [
    "start = torch.cuda.Event(enable_timing=True)\n",
    "end = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "start.record()\n",
    "ppl = eval_ppl(model, tokenizer)\n",
    "end.record()\n",
    "torch.cuda.synchronize()\n",
    "print(ppl)\n",
    "print(start.elapsed_time(end))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как изменилась перплексия и скорость расчета бенчмарка после квантизации?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверьте изменится ли скорость расчета бенчмарка если квантизовать только линейные слои в `mlp` блоках трансформера или в `self_attn` блоках трансформера."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "allocated: 7.29 gb\n",
      " reserved: 7.29 gb\n"
     ]
    }
   ],
   "source": [
    "del model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_benchmark(quantize_mlp, quantize_selfattn):\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_path,\n",
    "        torch_dtype=torch.float16,\n",
    "        trust_remote_code = True,\n",
    "        device_map = 'cuda:0'\n",
    "    )\n",
    "\n",
    "    replace_with_qlinear(model.model, quantize_mlp, quantize_selfattn, False)\n",
    "\n",
    "    start = torch.cuda.Event(enable_timing=True)\n",
    "    end = torch.cuda.Event(enable_timing=True)\n",
    "    \n",
    "    start.record()\n",
    "    ppl = eval_ppl(model, tokenizer)\n",
    "    end.record()\n",
    "    torch.cuda.synchronize()\n",
    "    print(ppl)\n",
    "    print(start.elapsed_time(end))\n",
    "\n",
    "    del model\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37737265393f446bb1921c5151de4b41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping quantizing layer layers.0.self_attn.q_proj\n",
      "skipping quantizing layer layers.0.self_attn.k_proj\n",
      "skipping quantizing layer layers.0.self_attn.v_proj\n",
      "skipping quantizing layer layers.0.self_attn.o_proj\n",
      "replace layer layers.0.mlp.gate_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.0.mlp.up_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.0.mlp.down_proj with W8A8Linear(14336, 4096, bias=False)\n",
      "skipping quantizing layer layers.1.self_attn.q_proj\n",
      "skipping quantizing layer layers.1.self_attn.k_proj\n",
      "skipping quantizing layer layers.1.self_attn.v_proj\n",
      "skipping quantizing layer layers.1.self_attn.o_proj\n",
      "replace layer layers.1.mlp.gate_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.1.mlp.up_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.1.mlp.down_proj with W8A8Linear(14336, 4096, bias=False)\n",
      "skipping quantizing layer layers.2.self_attn.q_proj\n",
      "skipping quantizing layer layers.2.self_attn.k_proj\n",
      "skipping quantizing layer layers.2.self_attn.v_proj\n",
      "skipping quantizing layer layers.2.self_attn.o_proj\n",
      "replace layer layers.2.mlp.gate_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.2.mlp.up_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.2.mlp.down_proj with W8A8Linear(14336, 4096, bias=False)\n",
      "skipping quantizing layer layers.3.self_attn.q_proj\n",
      "skipping quantizing layer layers.3.self_attn.k_proj\n",
      "skipping quantizing layer layers.3.self_attn.v_proj\n",
      "skipping quantizing layer layers.3.self_attn.o_proj\n",
      "replace layer layers.3.mlp.gate_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.3.mlp.up_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.3.mlp.down_proj with W8A8Linear(14336, 4096, bias=False)\n",
      "skipping quantizing layer layers.4.self_attn.q_proj\n",
      "skipping quantizing layer layers.4.self_attn.k_proj\n",
      "skipping quantizing layer layers.4.self_attn.v_proj\n",
      "skipping quantizing layer layers.4.self_attn.o_proj\n",
      "replace layer layers.4.mlp.gate_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.4.mlp.up_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.4.mlp.down_proj with W8A8Linear(14336, 4096, bias=False)\n",
      "skipping quantizing layer layers.5.self_attn.q_proj\n",
      "skipping quantizing layer layers.5.self_attn.k_proj\n",
      "skipping quantizing layer layers.5.self_attn.v_proj\n",
      "skipping quantizing layer layers.5.self_attn.o_proj\n",
      "replace layer layers.5.mlp.gate_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.5.mlp.up_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.5.mlp.down_proj with W8A8Linear(14336, 4096, bias=False)\n",
      "skipping quantizing layer layers.6.self_attn.q_proj\n",
      "skipping quantizing layer layers.6.self_attn.k_proj\n",
      "skipping quantizing layer layers.6.self_attn.v_proj\n",
      "skipping quantizing layer layers.6.self_attn.o_proj\n",
      "replace layer layers.6.mlp.gate_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.6.mlp.up_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.6.mlp.down_proj with W8A8Linear(14336, 4096, bias=False)\n",
      "skipping quantizing layer layers.7.self_attn.q_proj\n",
      "skipping quantizing layer layers.7.self_attn.k_proj\n",
      "skipping quantizing layer layers.7.self_attn.v_proj\n",
      "skipping quantizing layer layers.7.self_attn.o_proj\n",
      "replace layer layers.7.mlp.gate_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.7.mlp.up_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.7.mlp.down_proj with W8A8Linear(14336, 4096, bias=False)\n",
      "skipping quantizing layer layers.8.self_attn.q_proj\n",
      "skipping quantizing layer layers.8.self_attn.k_proj\n",
      "skipping quantizing layer layers.8.self_attn.v_proj\n",
      "skipping quantizing layer layers.8.self_attn.o_proj\n",
      "replace layer layers.8.mlp.gate_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.8.mlp.up_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.8.mlp.down_proj with W8A8Linear(14336, 4096, bias=False)\n",
      "skipping quantizing layer layers.9.self_attn.q_proj\n",
      "skipping quantizing layer layers.9.self_attn.k_proj\n",
      "skipping quantizing layer layers.9.self_attn.v_proj\n",
      "skipping quantizing layer layers.9.self_attn.o_proj\n",
      "replace layer layers.9.mlp.gate_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.9.mlp.up_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.9.mlp.down_proj with W8A8Linear(14336, 4096, bias=False)\n",
      "skipping quantizing layer layers.10.self_attn.q_proj\n",
      "skipping quantizing layer layers.10.self_attn.k_proj\n",
      "skipping quantizing layer layers.10.self_attn.v_proj\n",
      "skipping quantizing layer layers.10.self_attn.o_proj\n",
      "replace layer layers.10.mlp.gate_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.10.mlp.up_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.10.mlp.down_proj with W8A8Linear(14336, 4096, bias=False)\n",
      "skipping quantizing layer layers.11.self_attn.q_proj\n",
      "skipping quantizing layer layers.11.self_attn.k_proj\n",
      "skipping quantizing layer layers.11.self_attn.v_proj\n",
      "skipping quantizing layer layers.11.self_attn.o_proj\n",
      "replace layer layers.11.mlp.gate_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.11.mlp.up_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.11.mlp.down_proj with W8A8Linear(14336, 4096, bias=False)\n",
      "skipping quantizing layer layers.12.self_attn.q_proj\n",
      "skipping quantizing layer layers.12.self_attn.k_proj\n",
      "skipping quantizing layer layers.12.self_attn.v_proj\n",
      "skipping quantizing layer layers.12.self_attn.o_proj\n",
      "replace layer layers.12.mlp.gate_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.12.mlp.up_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.12.mlp.down_proj with W8A8Linear(14336, 4096, bias=False)\n",
      "skipping quantizing layer layers.13.self_attn.q_proj\n",
      "skipping quantizing layer layers.13.self_attn.k_proj\n",
      "skipping quantizing layer layers.13.self_attn.v_proj\n",
      "skipping quantizing layer layers.13.self_attn.o_proj\n",
      "replace layer layers.13.mlp.gate_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.13.mlp.up_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.13.mlp.down_proj with W8A8Linear(14336, 4096, bias=False)\n",
      "skipping quantizing layer layers.14.self_attn.q_proj\n",
      "skipping quantizing layer layers.14.self_attn.k_proj\n",
      "skipping quantizing layer layers.14.self_attn.v_proj\n",
      "skipping quantizing layer layers.14.self_attn.o_proj\n",
      "replace layer layers.14.mlp.gate_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.14.mlp.up_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.14.mlp.down_proj with W8A8Linear(14336, 4096, bias=False)\n",
      "skipping quantizing layer layers.15.self_attn.q_proj\n",
      "skipping quantizing layer layers.15.self_attn.k_proj\n",
      "skipping quantizing layer layers.15.self_attn.v_proj\n",
      "skipping quantizing layer layers.15.self_attn.o_proj\n",
      "replace layer layers.15.mlp.gate_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.15.mlp.up_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.15.mlp.down_proj with W8A8Linear(14336, 4096, bias=False)\n",
      "skipping quantizing layer layers.16.self_attn.q_proj\n",
      "skipping quantizing layer layers.16.self_attn.k_proj\n",
      "skipping quantizing layer layers.16.self_attn.v_proj\n",
      "skipping quantizing layer layers.16.self_attn.o_proj\n",
      "replace layer layers.16.mlp.gate_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.16.mlp.up_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.16.mlp.down_proj with W8A8Linear(14336, 4096, bias=False)\n",
      "skipping quantizing layer layers.17.self_attn.q_proj\n",
      "skipping quantizing layer layers.17.self_attn.k_proj\n",
      "skipping quantizing layer layers.17.self_attn.v_proj\n",
      "skipping quantizing layer layers.17.self_attn.o_proj\n",
      "replace layer layers.17.mlp.gate_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.17.mlp.up_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.17.mlp.down_proj with W8A8Linear(14336, 4096, bias=False)\n",
      "skipping quantizing layer layers.18.self_attn.q_proj\n",
      "skipping quantizing layer layers.18.self_attn.k_proj\n",
      "skipping quantizing layer layers.18.self_attn.v_proj\n",
      "skipping quantizing layer layers.18.self_attn.o_proj\n",
      "replace layer layers.18.mlp.gate_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.18.mlp.up_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.18.mlp.down_proj with W8A8Linear(14336, 4096, bias=False)\n",
      "skipping quantizing layer layers.19.self_attn.q_proj\n",
      "skipping quantizing layer layers.19.self_attn.k_proj\n",
      "skipping quantizing layer layers.19.self_attn.v_proj\n",
      "skipping quantizing layer layers.19.self_attn.o_proj\n",
      "replace layer layers.19.mlp.gate_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.19.mlp.up_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.19.mlp.down_proj with W8A8Linear(14336, 4096, bias=False)\n",
      "skipping quantizing layer layers.20.self_attn.q_proj\n",
      "skipping quantizing layer layers.20.self_attn.k_proj\n",
      "skipping quantizing layer layers.20.self_attn.v_proj\n",
      "skipping quantizing layer layers.20.self_attn.o_proj\n",
      "replace layer layers.20.mlp.gate_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.20.mlp.up_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.20.mlp.down_proj with W8A8Linear(14336, 4096, bias=False)\n",
      "skipping quantizing layer layers.21.self_attn.q_proj\n",
      "skipping quantizing layer layers.21.self_attn.k_proj\n",
      "skipping quantizing layer layers.21.self_attn.v_proj\n",
      "skipping quantizing layer layers.21.self_attn.o_proj\n",
      "replace layer layers.21.mlp.gate_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.21.mlp.up_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.21.mlp.down_proj with W8A8Linear(14336, 4096, bias=False)\n",
      "skipping quantizing layer layers.22.self_attn.q_proj\n",
      "skipping quantizing layer layers.22.self_attn.k_proj\n",
      "skipping quantizing layer layers.22.self_attn.v_proj\n",
      "skipping quantizing layer layers.22.self_attn.o_proj\n",
      "replace layer layers.22.mlp.gate_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.22.mlp.up_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.22.mlp.down_proj with W8A8Linear(14336, 4096, bias=False)\n",
      "skipping quantizing layer layers.23.self_attn.q_proj\n",
      "skipping quantizing layer layers.23.self_attn.k_proj\n",
      "skipping quantizing layer layers.23.self_attn.v_proj\n",
      "skipping quantizing layer layers.23.self_attn.o_proj\n",
      "replace layer layers.23.mlp.gate_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.23.mlp.up_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.23.mlp.down_proj with W8A8Linear(14336, 4096, bias=False)\n",
      "skipping quantizing layer layers.24.self_attn.q_proj\n",
      "skipping quantizing layer layers.24.self_attn.k_proj\n",
      "skipping quantizing layer layers.24.self_attn.v_proj\n",
      "skipping quantizing layer layers.24.self_attn.o_proj\n",
      "replace layer layers.24.mlp.gate_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.24.mlp.up_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.24.mlp.down_proj with W8A8Linear(14336, 4096, bias=False)\n",
      "skipping quantizing layer layers.25.self_attn.q_proj\n",
      "skipping quantizing layer layers.25.self_attn.k_proj\n",
      "skipping quantizing layer layers.25.self_attn.v_proj\n",
      "skipping quantizing layer layers.25.self_attn.o_proj\n",
      "replace layer layers.25.mlp.gate_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.25.mlp.up_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.25.mlp.down_proj with W8A8Linear(14336, 4096, bias=False)\n",
      "skipping quantizing layer layers.26.self_attn.q_proj\n",
      "skipping quantizing layer layers.26.self_attn.k_proj\n",
      "skipping quantizing layer layers.26.self_attn.v_proj\n",
      "skipping quantizing layer layers.26.self_attn.o_proj\n",
      "replace layer layers.26.mlp.gate_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.26.mlp.up_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.26.mlp.down_proj with W8A8Linear(14336, 4096, bias=False)\n",
      "skipping quantizing layer layers.27.self_attn.q_proj\n",
      "skipping quantizing layer layers.27.self_attn.k_proj\n",
      "skipping quantizing layer layers.27.self_attn.v_proj\n",
      "skipping quantizing layer layers.27.self_attn.o_proj\n",
      "replace layer layers.27.mlp.gate_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.27.mlp.up_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.27.mlp.down_proj with W8A8Linear(14336, 4096, bias=False)\n",
      "skipping quantizing layer layers.28.self_attn.q_proj\n",
      "skipping quantizing layer layers.28.self_attn.k_proj\n",
      "skipping quantizing layer layers.28.self_attn.v_proj\n",
      "skipping quantizing layer layers.28.self_attn.o_proj\n",
      "replace layer layers.28.mlp.gate_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.28.mlp.up_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.28.mlp.down_proj with W8A8Linear(14336, 4096, bias=False)\n",
      "skipping quantizing layer layers.29.self_attn.q_proj\n",
      "skipping quantizing layer layers.29.self_attn.k_proj\n",
      "skipping quantizing layer layers.29.self_attn.v_proj\n",
      "skipping quantizing layer layers.29.self_attn.o_proj\n",
      "replace layer layers.29.mlp.gate_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.29.mlp.up_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.29.mlp.down_proj with W8A8Linear(14336, 4096, bias=False)\n",
      "skipping quantizing layer layers.30.self_attn.q_proj\n",
      "skipping quantizing layer layers.30.self_attn.k_proj\n",
      "skipping quantizing layer layers.30.self_attn.v_proj\n",
      "skipping quantizing layer layers.30.self_attn.o_proj\n",
      "replace layer layers.30.mlp.gate_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.30.mlp.up_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.30.mlp.down_proj with W8A8Linear(14336, 4096, bias=False)\n",
      "skipping quantizing layer layers.31.self_attn.q_proj\n",
      "skipping quantizing layer layers.31.self_attn.k_proj\n",
      "skipping quantizing layer layers.31.self_attn.v_proj\n",
      "skipping quantizing layer layers.31.self_attn.o_proj\n",
      "replace layer layers.31.mlp.gate_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.31.mlp.up_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.31.mlp.down_proj with W8A8Linear(14336, 4096, bias=False)\n",
      "evaluating on wikitext2\n",
      "nsamples 163\n",
      "sample 0\n",
      "sample 50\n",
      "sample 100\n",
      "sample 150\n",
      "5.373754024505615\n",
      "119974.5859375\n"
     ]
    }
   ],
   "source": [
    "run_benchmark(True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "655053781a4c455bac1f367e95f32343",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "replace layer layers.0.self_attn.q_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.0.self_attn.k_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.0.self_attn.v_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.0.self_attn.o_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "skipping quantizing layer layers.0.mlp.gate_proj\n",
      "skipping quantizing layer layers.0.mlp.up_proj\n",
      "skipping quantizing layer layers.0.mlp.down_proj\n",
      "replace layer layers.1.self_attn.q_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.1.self_attn.k_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.1.self_attn.v_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.1.self_attn.o_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "skipping quantizing layer layers.1.mlp.gate_proj\n",
      "skipping quantizing layer layers.1.mlp.up_proj\n",
      "skipping quantizing layer layers.1.mlp.down_proj\n",
      "replace layer layers.2.self_attn.q_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.2.self_attn.k_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.2.self_attn.v_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.2.self_attn.o_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "skipping quantizing layer layers.2.mlp.gate_proj\n",
      "skipping quantizing layer layers.2.mlp.up_proj\n",
      "skipping quantizing layer layers.2.mlp.down_proj\n",
      "replace layer layers.3.self_attn.q_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.3.self_attn.k_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.3.self_attn.v_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.3.self_attn.o_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "skipping quantizing layer layers.3.mlp.gate_proj\n",
      "skipping quantizing layer layers.3.mlp.up_proj\n",
      "skipping quantizing layer layers.3.mlp.down_proj\n",
      "replace layer layers.4.self_attn.q_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.4.self_attn.k_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.4.self_attn.v_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.4.self_attn.o_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "skipping quantizing layer layers.4.mlp.gate_proj\n",
      "skipping quantizing layer layers.4.mlp.up_proj\n",
      "skipping quantizing layer layers.4.mlp.down_proj\n",
      "replace layer layers.5.self_attn.q_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.5.self_attn.k_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.5.self_attn.v_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.5.self_attn.o_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "skipping quantizing layer layers.5.mlp.gate_proj\n",
      "skipping quantizing layer layers.5.mlp.up_proj\n",
      "skipping quantizing layer layers.5.mlp.down_proj\n",
      "replace layer layers.6.self_attn.q_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.6.self_attn.k_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.6.self_attn.v_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.6.self_attn.o_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "skipping quantizing layer layers.6.mlp.gate_proj\n",
      "skipping quantizing layer layers.6.mlp.up_proj\n",
      "skipping quantizing layer layers.6.mlp.down_proj\n",
      "replace layer layers.7.self_attn.q_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.7.self_attn.k_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.7.self_attn.v_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.7.self_attn.o_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "skipping quantizing layer layers.7.mlp.gate_proj\n",
      "skipping quantizing layer layers.7.mlp.up_proj\n",
      "skipping quantizing layer layers.7.mlp.down_proj\n",
      "replace layer layers.8.self_attn.q_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.8.self_attn.k_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.8.self_attn.v_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.8.self_attn.o_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "skipping quantizing layer layers.8.mlp.gate_proj\n",
      "skipping quantizing layer layers.8.mlp.up_proj\n",
      "skipping quantizing layer layers.8.mlp.down_proj\n",
      "replace layer layers.9.self_attn.q_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.9.self_attn.k_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.9.self_attn.v_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.9.self_attn.o_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "skipping quantizing layer layers.9.mlp.gate_proj\n",
      "skipping quantizing layer layers.9.mlp.up_proj\n",
      "skipping quantizing layer layers.9.mlp.down_proj\n",
      "replace layer layers.10.self_attn.q_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.10.self_attn.k_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.10.self_attn.v_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.10.self_attn.o_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "skipping quantizing layer layers.10.mlp.gate_proj\n",
      "skipping quantizing layer layers.10.mlp.up_proj\n",
      "skipping quantizing layer layers.10.mlp.down_proj\n",
      "replace layer layers.11.self_attn.q_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.11.self_attn.k_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.11.self_attn.v_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.11.self_attn.o_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "skipping quantizing layer layers.11.mlp.gate_proj\n",
      "skipping quantizing layer layers.11.mlp.up_proj\n",
      "skipping quantizing layer layers.11.mlp.down_proj\n",
      "replace layer layers.12.self_attn.q_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.12.self_attn.k_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.12.self_attn.v_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.12.self_attn.o_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "skipping quantizing layer layers.12.mlp.gate_proj\n",
      "skipping quantizing layer layers.12.mlp.up_proj\n",
      "skipping quantizing layer layers.12.mlp.down_proj\n",
      "replace layer layers.13.self_attn.q_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.13.self_attn.k_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.13.self_attn.v_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.13.self_attn.o_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "skipping quantizing layer layers.13.mlp.gate_proj\n",
      "skipping quantizing layer layers.13.mlp.up_proj\n",
      "skipping quantizing layer layers.13.mlp.down_proj\n",
      "replace layer layers.14.self_attn.q_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.14.self_attn.k_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.14.self_attn.v_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.14.self_attn.o_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "skipping quantizing layer layers.14.mlp.gate_proj\n",
      "skipping quantizing layer layers.14.mlp.up_proj\n",
      "skipping quantizing layer layers.14.mlp.down_proj\n",
      "replace layer layers.15.self_attn.q_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.15.self_attn.k_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.15.self_attn.v_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.15.self_attn.o_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "skipping quantizing layer layers.15.mlp.gate_proj\n",
      "skipping quantizing layer layers.15.mlp.up_proj\n",
      "skipping quantizing layer layers.15.mlp.down_proj\n",
      "replace layer layers.16.self_attn.q_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.16.self_attn.k_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.16.self_attn.v_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.16.self_attn.o_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "skipping quantizing layer layers.16.mlp.gate_proj\n",
      "skipping quantizing layer layers.16.mlp.up_proj\n",
      "skipping quantizing layer layers.16.mlp.down_proj\n",
      "replace layer layers.17.self_attn.q_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.17.self_attn.k_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.17.self_attn.v_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.17.self_attn.o_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "skipping quantizing layer layers.17.mlp.gate_proj\n",
      "skipping quantizing layer layers.17.mlp.up_proj\n",
      "skipping quantizing layer layers.17.mlp.down_proj\n",
      "replace layer layers.18.self_attn.q_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.18.self_attn.k_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.18.self_attn.v_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.18.self_attn.o_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "skipping quantizing layer layers.18.mlp.gate_proj\n",
      "skipping quantizing layer layers.18.mlp.up_proj\n",
      "skipping quantizing layer layers.18.mlp.down_proj\n",
      "replace layer layers.19.self_attn.q_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.19.self_attn.k_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.19.self_attn.v_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.19.self_attn.o_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "skipping quantizing layer layers.19.mlp.gate_proj\n",
      "skipping quantizing layer layers.19.mlp.up_proj\n",
      "skipping quantizing layer layers.19.mlp.down_proj\n",
      "replace layer layers.20.self_attn.q_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.20.self_attn.k_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.20.self_attn.v_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.20.self_attn.o_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "skipping quantizing layer layers.20.mlp.gate_proj\n",
      "skipping quantizing layer layers.20.mlp.up_proj\n",
      "skipping quantizing layer layers.20.mlp.down_proj\n",
      "replace layer layers.21.self_attn.q_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.21.self_attn.k_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.21.self_attn.v_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.21.self_attn.o_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "skipping quantizing layer layers.21.mlp.gate_proj\n",
      "skipping quantizing layer layers.21.mlp.up_proj\n",
      "skipping quantizing layer layers.21.mlp.down_proj\n",
      "replace layer layers.22.self_attn.q_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.22.self_attn.k_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.22.self_attn.v_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.22.self_attn.o_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "skipping quantizing layer layers.22.mlp.gate_proj\n",
      "skipping quantizing layer layers.22.mlp.up_proj\n",
      "skipping quantizing layer layers.22.mlp.down_proj\n",
      "replace layer layers.23.self_attn.q_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.23.self_attn.k_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.23.self_attn.v_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.23.self_attn.o_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "skipping quantizing layer layers.23.mlp.gate_proj\n",
      "skipping quantizing layer layers.23.mlp.up_proj\n",
      "skipping quantizing layer layers.23.mlp.down_proj\n",
      "replace layer layers.24.self_attn.q_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.24.self_attn.k_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.24.self_attn.v_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.24.self_attn.o_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "skipping quantizing layer layers.24.mlp.gate_proj\n",
      "skipping quantizing layer layers.24.mlp.up_proj\n",
      "skipping quantizing layer layers.24.mlp.down_proj\n",
      "replace layer layers.25.self_attn.q_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.25.self_attn.k_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.25.self_attn.v_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.25.self_attn.o_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "skipping quantizing layer layers.25.mlp.gate_proj\n",
      "skipping quantizing layer layers.25.mlp.up_proj\n",
      "skipping quantizing layer layers.25.mlp.down_proj\n",
      "replace layer layers.26.self_attn.q_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.26.self_attn.k_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.26.self_attn.v_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.26.self_attn.o_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "skipping quantizing layer layers.26.mlp.gate_proj\n",
      "skipping quantizing layer layers.26.mlp.up_proj\n",
      "skipping quantizing layer layers.26.mlp.down_proj\n",
      "replace layer layers.27.self_attn.q_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.27.self_attn.k_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.27.self_attn.v_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.27.self_attn.o_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "skipping quantizing layer layers.27.mlp.gate_proj\n",
      "skipping quantizing layer layers.27.mlp.up_proj\n",
      "skipping quantizing layer layers.27.mlp.down_proj\n",
      "replace layer layers.28.self_attn.q_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.28.self_attn.k_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.28.self_attn.v_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.28.self_attn.o_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "skipping quantizing layer layers.28.mlp.gate_proj\n",
      "skipping quantizing layer layers.28.mlp.up_proj\n",
      "skipping quantizing layer layers.28.mlp.down_proj\n",
      "replace layer layers.29.self_attn.q_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.29.self_attn.k_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.29.self_attn.v_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.29.self_attn.o_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "skipping quantizing layer layers.29.mlp.gate_proj\n",
      "skipping quantizing layer layers.29.mlp.up_proj\n",
      "skipping quantizing layer layers.29.mlp.down_proj\n",
      "replace layer layers.30.self_attn.q_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.30.self_attn.k_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.30.self_attn.v_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.30.self_attn.o_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "skipping quantizing layer layers.30.mlp.gate_proj\n",
      "skipping quantizing layer layers.30.mlp.up_proj\n",
      "skipping quantizing layer layers.30.mlp.down_proj\n",
      "replace layer layers.31.self_attn.q_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.31.self_attn.k_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.31.self_attn.v_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.31.self_attn.o_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "skipping quantizing layer layers.31.mlp.gate_proj\n",
      "skipping quantizing layer layers.31.mlp.up_proj\n",
      "skipping quantizing layer layers.31.mlp.down_proj\n",
      "evaluating on wikitext2\n",
      "nsamples 163\n",
      "sample 0\n",
      "sample 50\n",
      "sample 100\n",
      "sample 150\n",
      "5.331487655639648\n",
      "180279.59375\n"
     ]
    }
   ],
   "source": [
    "run_benchmark(False, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "allocated: 0.01 gb\n",
      " reserved: 0.01 gb\n"
     ]
    }
   ],
   "source": [
    "print_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f323606c0b454a128b2e0787c936c31f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "replace layer layers.0.self_attn.q_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.0.self_attn.k_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.0.self_attn.v_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.0.self_attn.o_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.0.mlp.gate_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.0.mlp.up_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.0.mlp.down_proj with W8A8Linear(14336, 4096, bias=False)\n",
      "replace layer layers.1.self_attn.q_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.1.self_attn.k_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.1.self_attn.v_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.1.self_attn.o_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.1.mlp.gate_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.1.mlp.up_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.1.mlp.down_proj with W8A8Linear(14336, 4096, bias=False)\n",
      "replace layer layers.2.self_attn.q_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.2.self_attn.k_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.2.self_attn.v_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.2.self_attn.o_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.2.mlp.gate_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.2.mlp.up_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.2.mlp.down_proj with W8A8Linear(14336, 4096, bias=False)\n",
      "replace layer layers.3.self_attn.q_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.3.self_attn.k_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.3.self_attn.v_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.3.self_attn.o_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.3.mlp.gate_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.3.mlp.up_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.3.mlp.down_proj with W8A8Linear(14336, 4096, bias=False)\n",
      "replace layer layers.4.self_attn.q_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.4.self_attn.k_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.4.self_attn.v_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.4.self_attn.o_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.4.mlp.gate_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.4.mlp.up_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.4.mlp.down_proj with W8A8Linear(14336, 4096, bias=False)\n",
      "replace layer layers.5.self_attn.q_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.5.self_attn.k_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.5.self_attn.v_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.5.self_attn.o_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.5.mlp.gate_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.5.mlp.up_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.5.mlp.down_proj with W8A8Linear(14336, 4096, bias=False)\n",
      "replace layer layers.6.self_attn.q_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.6.self_attn.k_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.6.self_attn.v_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.6.self_attn.o_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.6.mlp.gate_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.6.mlp.up_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.6.mlp.down_proj with W8A8Linear(14336, 4096, bias=False)\n",
      "replace layer layers.7.self_attn.q_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.7.self_attn.k_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.7.self_attn.v_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.7.self_attn.o_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.7.mlp.gate_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.7.mlp.up_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.7.mlp.down_proj with W8A8Linear(14336, 4096, bias=False)\n",
      "replace layer layers.8.self_attn.q_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.8.self_attn.k_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.8.self_attn.v_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.8.self_attn.o_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.8.mlp.gate_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.8.mlp.up_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.8.mlp.down_proj with W8A8Linear(14336, 4096, bias=False)\n",
      "replace layer layers.9.self_attn.q_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.9.self_attn.k_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.9.self_attn.v_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.9.self_attn.o_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.9.mlp.gate_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.9.mlp.up_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.9.mlp.down_proj with W8A8Linear(14336, 4096, bias=False)\n",
      "replace layer layers.10.self_attn.q_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.10.self_attn.k_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.10.self_attn.v_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.10.self_attn.o_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.10.mlp.gate_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.10.mlp.up_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.10.mlp.down_proj with W8A8Linear(14336, 4096, bias=False)\n",
      "replace layer layers.11.self_attn.q_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.11.self_attn.k_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.11.self_attn.v_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.11.self_attn.o_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.11.mlp.gate_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.11.mlp.up_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.11.mlp.down_proj with W8A8Linear(14336, 4096, bias=False)\n",
      "replace layer layers.12.self_attn.q_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.12.self_attn.k_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.12.self_attn.v_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.12.self_attn.o_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.12.mlp.gate_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.12.mlp.up_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.12.mlp.down_proj with W8A8Linear(14336, 4096, bias=False)\n",
      "replace layer layers.13.self_attn.q_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.13.self_attn.k_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.13.self_attn.v_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.13.self_attn.o_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.13.mlp.gate_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.13.mlp.up_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.13.mlp.down_proj with W8A8Linear(14336, 4096, bias=False)\n",
      "replace layer layers.14.self_attn.q_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.14.self_attn.k_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.14.self_attn.v_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.14.self_attn.o_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.14.mlp.gate_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.14.mlp.up_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.14.mlp.down_proj with W8A8Linear(14336, 4096, bias=False)\n",
      "replace layer layers.15.self_attn.q_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.15.self_attn.k_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.15.self_attn.v_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.15.self_attn.o_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.15.mlp.gate_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.15.mlp.up_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.15.mlp.down_proj with W8A8Linear(14336, 4096, bias=False)\n",
      "replace layer layers.16.self_attn.q_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.16.self_attn.k_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.16.self_attn.v_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.16.self_attn.o_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.16.mlp.gate_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.16.mlp.up_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.16.mlp.down_proj with W8A8Linear(14336, 4096, bias=False)\n",
      "replace layer layers.17.self_attn.q_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.17.self_attn.k_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.17.self_attn.v_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.17.self_attn.o_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.17.mlp.gate_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.17.mlp.up_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.17.mlp.down_proj with W8A8Linear(14336, 4096, bias=False)\n",
      "replace layer layers.18.self_attn.q_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.18.self_attn.k_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.18.self_attn.v_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.18.self_attn.o_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.18.mlp.gate_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.18.mlp.up_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.18.mlp.down_proj with W8A8Linear(14336, 4096, bias=False)\n",
      "replace layer layers.19.self_attn.q_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.19.self_attn.k_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.19.self_attn.v_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.19.self_attn.o_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.19.mlp.gate_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.19.mlp.up_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.19.mlp.down_proj with W8A8Linear(14336, 4096, bias=False)\n",
      "replace layer layers.20.self_attn.q_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.20.self_attn.k_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.20.self_attn.v_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.20.self_attn.o_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.20.mlp.gate_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.20.mlp.up_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.20.mlp.down_proj with W8A8Linear(14336, 4096, bias=False)\n",
      "replace layer layers.21.self_attn.q_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.21.self_attn.k_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.21.self_attn.v_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.21.self_attn.o_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.21.mlp.gate_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.21.mlp.up_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.21.mlp.down_proj with W8A8Linear(14336, 4096, bias=False)\n",
      "replace layer layers.22.self_attn.q_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.22.self_attn.k_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.22.self_attn.v_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.22.self_attn.o_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.22.mlp.gate_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.22.mlp.up_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.22.mlp.down_proj with W8A8Linear(14336, 4096, bias=False)\n",
      "replace layer layers.23.self_attn.q_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.23.self_attn.k_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.23.self_attn.v_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.23.self_attn.o_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.23.mlp.gate_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.23.mlp.up_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.23.mlp.down_proj with W8A8Linear(14336, 4096, bias=False)\n",
      "replace layer layers.24.self_attn.q_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.24.self_attn.k_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.24.self_attn.v_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.24.self_attn.o_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.24.mlp.gate_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.24.mlp.up_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.24.mlp.down_proj with W8A8Linear(14336, 4096, bias=False)\n",
      "replace layer layers.25.self_attn.q_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.25.self_attn.k_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.25.self_attn.v_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.25.self_attn.o_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.25.mlp.gate_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.25.mlp.up_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.25.mlp.down_proj with W8A8Linear(14336, 4096, bias=False)\n",
      "replace layer layers.26.self_attn.q_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.26.self_attn.k_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.26.self_attn.v_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.26.self_attn.o_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.26.mlp.gate_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.26.mlp.up_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.26.mlp.down_proj with W8A8Linear(14336, 4096, bias=False)\n",
      "replace layer layers.27.self_attn.q_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.27.self_attn.k_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.27.self_attn.v_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.27.self_attn.o_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.27.mlp.gate_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.27.mlp.up_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.27.mlp.down_proj with W8A8Linear(14336, 4096, bias=False)\n",
      "replace layer layers.28.self_attn.q_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.28.self_attn.k_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.28.self_attn.v_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.28.self_attn.o_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.28.mlp.gate_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.28.mlp.up_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.28.mlp.down_proj with W8A8Linear(14336, 4096, bias=False)\n",
      "replace layer layers.29.self_attn.q_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.29.self_attn.k_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.29.self_attn.v_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.29.self_attn.o_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.29.mlp.gate_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.29.mlp.up_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.29.mlp.down_proj with W8A8Linear(14336, 4096, bias=False)\n",
      "replace layer layers.30.self_attn.q_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.30.self_attn.k_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.30.self_attn.v_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.30.self_attn.o_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.30.mlp.gate_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.30.mlp.up_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.30.mlp.down_proj with W8A8Linear(14336, 4096, bias=False)\n",
      "replace layer layers.31.self_attn.q_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.31.self_attn.k_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.31.self_attn.v_proj with W8A8Linear(4096, 1024, bias=False)\n",
      "replace layer layers.31.self_attn.o_proj with W8A8Linear(4096, 4096, bias=False)\n",
      "replace layer layers.31.mlp.gate_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.31.mlp.up_proj with W8A8Linear(4096, 14336, bias=False)\n",
      "replace layer layers.31.mlp.down_proj with W8A8Linear(14336, 4096, bias=False)\n",
      "evaluating on wikitext2\n",
      "nsamples 163\n",
      "sample 0\n",
      "sample 50\n",
      "sample 100\n",
      "sample 150\n",
      "5.3877129554748535\n",
      "116004.625\n"
     ]
    }
   ],
   "source": [
    "run_benchmark(True, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "allocated: 0.01 gb\n",
      " reserved: 0.01 gb\n"
     ]
    }
   ],
   "source": [
    "print_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e25501836027475594289cae9e14e004",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping quantizing layer layers.0.self_attn.q_proj\n",
      "skipping quantizing layer layers.0.self_attn.k_proj\n",
      "skipping quantizing layer layers.0.self_attn.v_proj\n",
      "skipping quantizing layer layers.0.self_attn.o_proj\n",
      "skipping quantizing layer layers.0.mlp.gate_proj\n",
      "skipping quantizing layer layers.0.mlp.up_proj\n",
      "skipping quantizing layer layers.0.mlp.down_proj\n",
      "skipping quantizing layer layers.1.self_attn.q_proj\n",
      "skipping quantizing layer layers.1.self_attn.k_proj\n",
      "skipping quantizing layer layers.1.self_attn.v_proj\n",
      "skipping quantizing layer layers.1.self_attn.o_proj\n",
      "skipping quantizing layer layers.1.mlp.gate_proj\n",
      "skipping quantizing layer layers.1.mlp.up_proj\n",
      "skipping quantizing layer layers.1.mlp.down_proj\n",
      "skipping quantizing layer layers.2.self_attn.q_proj\n",
      "skipping quantizing layer layers.2.self_attn.k_proj\n",
      "skipping quantizing layer layers.2.self_attn.v_proj\n",
      "skipping quantizing layer layers.2.self_attn.o_proj\n",
      "skipping quantizing layer layers.2.mlp.gate_proj\n",
      "skipping quantizing layer layers.2.mlp.up_proj\n",
      "skipping quantizing layer layers.2.mlp.down_proj\n",
      "skipping quantizing layer layers.3.self_attn.q_proj\n",
      "skipping quantizing layer layers.3.self_attn.k_proj\n",
      "skipping quantizing layer layers.3.self_attn.v_proj\n",
      "skipping quantizing layer layers.3.self_attn.o_proj\n",
      "skipping quantizing layer layers.3.mlp.gate_proj\n",
      "skipping quantizing layer layers.3.mlp.up_proj\n",
      "skipping quantizing layer layers.3.mlp.down_proj\n",
      "skipping quantizing layer layers.4.self_attn.q_proj\n",
      "skipping quantizing layer layers.4.self_attn.k_proj\n",
      "skipping quantizing layer layers.4.self_attn.v_proj\n",
      "skipping quantizing layer layers.4.self_attn.o_proj\n",
      "skipping quantizing layer layers.4.mlp.gate_proj\n",
      "skipping quantizing layer layers.4.mlp.up_proj\n",
      "skipping quantizing layer layers.4.mlp.down_proj\n",
      "skipping quantizing layer layers.5.self_attn.q_proj\n",
      "skipping quantizing layer layers.5.self_attn.k_proj\n",
      "skipping quantizing layer layers.5.self_attn.v_proj\n",
      "skipping quantizing layer layers.5.self_attn.o_proj\n",
      "skipping quantizing layer layers.5.mlp.gate_proj\n",
      "skipping quantizing layer layers.5.mlp.up_proj\n",
      "skipping quantizing layer layers.5.mlp.down_proj\n",
      "skipping quantizing layer layers.6.self_attn.q_proj\n",
      "skipping quantizing layer layers.6.self_attn.k_proj\n",
      "skipping quantizing layer layers.6.self_attn.v_proj\n",
      "skipping quantizing layer layers.6.self_attn.o_proj\n",
      "skipping quantizing layer layers.6.mlp.gate_proj\n",
      "skipping quantizing layer layers.6.mlp.up_proj\n",
      "skipping quantizing layer layers.6.mlp.down_proj\n",
      "skipping quantizing layer layers.7.self_attn.q_proj\n",
      "skipping quantizing layer layers.7.self_attn.k_proj\n",
      "skipping quantizing layer layers.7.self_attn.v_proj\n",
      "skipping quantizing layer layers.7.self_attn.o_proj\n",
      "skipping quantizing layer layers.7.mlp.gate_proj\n",
      "skipping quantizing layer layers.7.mlp.up_proj\n",
      "skipping quantizing layer layers.7.mlp.down_proj\n",
      "skipping quantizing layer layers.8.self_attn.q_proj\n",
      "skipping quantizing layer layers.8.self_attn.k_proj\n",
      "skipping quantizing layer layers.8.self_attn.v_proj\n",
      "skipping quantizing layer layers.8.self_attn.o_proj\n",
      "skipping quantizing layer layers.8.mlp.gate_proj\n",
      "skipping quantizing layer layers.8.mlp.up_proj\n",
      "skipping quantizing layer layers.8.mlp.down_proj\n",
      "skipping quantizing layer layers.9.self_attn.q_proj\n",
      "skipping quantizing layer layers.9.self_attn.k_proj\n",
      "skipping quantizing layer layers.9.self_attn.v_proj\n",
      "skipping quantizing layer layers.9.self_attn.o_proj\n",
      "skipping quantizing layer layers.9.mlp.gate_proj\n",
      "skipping quantizing layer layers.9.mlp.up_proj\n",
      "skipping quantizing layer layers.9.mlp.down_proj\n",
      "skipping quantizing layer layers.10.self_attn.q_proj\n",
      "skipping quantizing layer layers.10.self_attn.k_proj\n",
      "skipping quantizing layer layers.10.self_attn.v_proj\n",
      "skipping quantizing layer layers.10.self_attn.o_proj\n",
      "skipping quantizing layer layers.10.mlp.gate_proj\n",
      "skipping quantizing layer layers.10.mlp.up_proj\n",
      "skipping quantizing layer layers.10.mlp.down_proj\n",
      "skipping quantizing layer layers.11.self_attn.q_proj\n",
      "skipping quantizing layer layers.11.self_attn.k_proj\n",
      "skipping quantizing layer layers.11.self_attn.v_proj\n",
      "skipping quantizing layer layers.11.self_attn.o_proj\n",
      "skipping quantizing layer layers.11.mlp.gate_proj\n",
      "skipping quantizing layer layers.11.mlp.up_proj\n",
      "skipping quantizing layer layers.11.mlp.down_proj\n",
      "skipping quantizing layer layers.12.self_attn.q_proj\n",
      "skipping quantizing layer layers.12.self_attn.k_proj\n",
      "skipping quantizing layer layers.12.self_attn.v_proj\n",
      "skipping quantizing layer layers.12.self_attn.o_proj\n",
      "skipping quantizing layer layers.12.mlp.gate_proj\n",
      "skipping quantizing layer layers.12.mlp.up_proj\n",
      "skipping quantizing layer layers.12.mlp.down_proj\n",
      "skipping quantizing layer layers.13.self_attn.q_proj\n",
      "skipping quantizing layer layers.13.self_attn.k_proj\n",
      "skipping quantizing layer layers.13.self_attn.v_proj\n",
      "skipping quantizing layer layers.13.self_attn.o_proj\n",
      "skipping quantizing layer layers.13.mlp.gate_proj\n",
      "skipping quantizing layer layers.13.mlp.up_proj\n",
      "skipping quantizing layer layers.13.mlp.down_proj\n",
      "skipping quantizing layer layers.14.self_attn.q_proj\n",
      "skipping quantizing layer layers.14.self_attn.k_proj\n",
      "skipping quantizing layer layers.14.self_attn.v_proj\n",
      "skipping quantizing layer layers.14.self_attn.o_proj\n",
      "skipping quantizing layer layers.14.mlp.gate_proj\n",
      "skipping quantizing layer layers.14.mlp.up_proj\n",
      "skipping quantizing layer layers.14.mlp.down_proj\n",
      "skipping quantizing layer layers.15.self_attn.q_proj\n",
      "skipping quantizing layer layers.15.self_attn.k_proj\n",
      "skipping quantizing layer layers.15.self_attn.v_proj\n",
      "skipping quantizing layer layers.15.self_attn.o_proj\n",
      "skipping quantizing layer layers.15.mlp.gate_proj\n",
      "skipping quantizing layer layers.15.mlp.up_proj\n",
      "skipping quantizing layer layers.15.mlp.down_proj\n",
      "skipping quantizing layer layers.16.self_attn.q_proj\n",
      "skipping quantizing layer layers.16.self_attn.k_proj\n",
      "skipping quantizing layer layers.16.self_attn.v_proj\n",
      "skipping quantizing layer layers.16.self_attn.o_proj\n",
      "skipping quantizing layer layers.16.mlp.gate_proj\n",
      "skipping quantizing layer layers.16.mlp.up_proj\n",
      "skipping quantizing layer layers.16.mlp.down_proj\n",
      "skipping quantizing layer layers.17.self_attn.q_proj\n",
      "skipping quantizing layer layers.17.self_attn.k_proj\n",
      "skipping quantizing layer layers.17.self_attn.v_proj\n",
      "skipping quantizing layer layers.17.self_attn.o_proj\n",
      "skipping quantizing layer layers.17.mlp.gate_proj\n",
      "skipping quantizing layer layers.17.mlp.up_proj\n",
      "skipping quantizing layer layers.17.mlp.down_proj\n",
      "skipping quantizing layer layers.18.self_attn.q_proj\n",
      "skipping quantizing layer layers.18.self_attn.k_proj\n",
      "skipping quantizing layer layers.18.self_attn.v_proj\n",
      "skipping quantizing layer layers.18.self_attn.o_proj\n",
      "skipping quantizing layer layers.18.mlp.gate_proj\n",
      "skipping quantizing layer layers.18.mlp.up_proj\n",
      "skipping quantizing layer layers.18.mlp.down_proj\n",
      "skipping quantizing layer layers.19.self_attn.q_proj\n",
      "skipping quantizing layer layers.19.self_attn.k_proj\n",
      "skipping quantizing layer layers.19.self_attn.v_proj\n",
      "skipping quantizing layer layers.19.self_attn.o_proj\n",
      "skipping quantizing layer layers.19.mlp.gate_proj\n",
      "skipping quantizing layer layers.19.mlp.up_proj\n",
      "skipping quantizing layer layers.19.mlp.down_proj\n",
      "skipping quantizing layer layers.20.self_attn.q_proj\n",
      "skipping quantizing layer layers.20.self_attn.k_proj\n",
      "skipping quantizing layer layers.20.self_attn.v_proj\n",
      "skipping quantizing layer layers.20.self_attn.o_proj\n",
      "skipping quantizing layer layers.20.mlp.gate_proj\n",
      "skipping quantizing layer layers.20.mlp.up_proj\n",
      "skipping quantizing layer layers.20.mlp.down_proj\n",
      "skipping quantizing layer layers.21.self_attn.q_proj\n",
      "skipping quantizing layer layers.21.self_attn.k_proj\n",
      "skipping quantizing layer layers.21.self_attn.v_proj\n",
      "skipping quantizing layer layers.21.self_attn.o_proj\n",
      "skipping quantizing layer layers.21.mlp.gate_proj\n",
      "skipping quantizing layer layers.21.mlp.up_proj\n",
      "skipping quantizing layer layers.21.mlp.down_proj\n",
      "skipping quantizing layer layers.22.self_attn.q_proj\n",
      "skipping quantizing layer layers.22.self_attn.k_proj\n",
      "skipping quantizing layer layers.22.self_attn.v_proj\n",
      "skipping quantizing layer layers.22.self_attn.o_proj\n",
      "skipping quantizing layer layers.22.mlp.gate_proj\n",
      "skipping quantizing layer layers.22.mlp.up_proj\n",
      "skipping quantizing layer layers.22.mlp.down_proj\n",
      "skipping quantizing layer layers.23.self_attn.q_proj\n",
      "skipping quantizing layer layers.23.self_attn.k_proj\n",
      "skipping quantizing layer layers.23.self_attn.v_proj\n",
      "skipping quantizing layer layers.23.self_attn.o_proj\n",
      "skipping quantizing layer layers.23.mlp.gate_proj\n",
      "skipping quantizing layer layers.23.mlp.up_proj\n",
      "skipping quantizing layer layers.23.mlp.down_proj\n",
      "skipping quantizing layer layers.24.self_attn.q_proj\n",
      "skipping quantizing layer layers.24.self_attn.k_proj\n",
      "skipping quantizing layer layers.24.self_attn.v_proj\n",
      "skipping quantizing layer layers.24.self_attn.o_proj\n",
      "skipping quantizing layer layers.24.mlp.gate_proj\n",
      "skipping quantizing layer layers.24.mlp.up_proj\n",
      "skipping quantizing layer layers.24.mlp.down_proj\n",
      "skipping quantizing layer layers.25.self_attn.q_proj\n",
      "skipping quantizing layer layers.25.self_attn.k_proj\n",
      "skipping quantizing layer layers.25.self_attn.v_proj\n",
      "skipping quantizing layer layers.25.self_attn.o_proj\n",
      "skipping quantizing layer layers.25.mlp.gate_proj\n",
      "skipping quantizing layer layers.25.mlp.up_proj\n",
      "skipping quantizing layer layers.25.mlp.down_proj\n",
      "skipping quantizing layer layers.26.self_attn.q_proj\n",
      "skipping quantizing layer layers.26.self_attn.k_proj\n",
      "skipping quantizing layer layers.26.self_attn.v_proj\n",
      "skipping quantizing layer layers.26.self_attn.o_proj\n",
      "skipping quantizing layer layers.26.mlp.gate_proj\n",
      "skipping quantizing layer layers.26.mlp.up_proj\n",
      "skipping quantizing layer layers.26.mlp.down_proj\n",
      "skipping quantizing layer layers.27.self_attn.q_proj\n",
      "skipping quantizing layer layers.27.self_attn.k_proj\n",
      "skipping quantizing layer layers.27.self_attn.v_proj\n",
      "skipping quantizing layer layers.27.self_attn.o_proj\n",
      "skipping quantizing layer layers.27.mlp.gate_proj\n",
      "skipping quantizing layer layers.27.mlp.up_proj\n",
      "skipping quantizing layer layers.27.mlp.down_proj\n",
      "skipping quantizing layer layers.28.self_attn.q_proj\n",
      "skipping quantizing layer layers.28.self_attn.k_proj\n",
      "skipping quantizing layer layers.28.self_attn.v_proj\n",
      "skipping quantizing layer layers.28.self_attn.o_proj\n",
      "skipping quantizing layer layers.28.mlp.gate_proj\n",
      "skipping quantizing layer layers.28.mlp.up_proj\n",
      "skipping quantizing layer layers.28.mlp.down_proj\n",
      "skipping quantizing layer layers.29.self_attn.q_proj\n",
      "skipping quantizing layer layers.29.self_attn.k_proj\n",
      "skipping quantizing layer layers.29.self_attn.v_proj\n",
      "skipping quantizing layer layers.29.self_attn.o_proj\n",
      "skipping quantizing layer layers.29.mlp.gate_proj\n",
      "skipping quantizing layer layers.29.mlp.up_proj\n",
      "skipping quantizing layer layers.29.mlp.down_proj\n",
      "skipping quantizing layer layers.30.self_attn.q_proj\n",
      "skipping quantizing layer layers.30.self_attn.k_proj\n",
      "skipping quantizing layer layers.30.self_attn.v_proj\n",
      "skipping quantizing layer layers.30.self_attn.o_proj\n",
      "skipping quantizing layer layers.30.mlp.gate_proj\n",
      "skipping quantizing layer layers.30.mlp.up_proj\n",
      "skipping quantizing layer layers.30.mlp.down_proj\n",
      "skipping quantizing layer layers.31.self_attn.q_proj\n",
      "skipping quantizing layer layers.31.self_attn.k_proj\n",
      "skipping quantizing layer layers.31.self_attn.v_proj\n",
      "skipping quantizing layer layers.31.self_attn.o_proj\n",
      "skipping quantizing layer layers.31.mlp.gate_proj\n",
      "skipping quantizing layer layers.31.mlp.up_proj\n",
      "skipping quantizing layer layers.31.mlp.down_proj\n",
      "evaluating on wikitext2\n",
      "nsamples 163\n",
      "sample 0\n",
      "sample 50\n",
      "sample 100\n",
      "sample 150\n",
      "5.3174519538879395\n",
      "135558.484375\n"
     ]
    }
   ],
   "source": [
    "run_benchmark(False, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
